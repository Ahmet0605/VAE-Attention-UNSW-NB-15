{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T16:38:22.991668Z",
     "start_time": "2025-10-29T16:38:19.403097Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# === 1️⃣ Veri yükleme ===\n",
    "train_data_df = pd.read_csv('../data/UNSW_NB15_training-set.csv')\n",
    "test_data_df = pd.read_csv('../data/UNSW_NB15_testing-set.csv')\n",
    "\n",
    "\n",
    "# === 2️⃣ Verileri birleştir ===\n",
    "data_df = pd.concat([train_data_df, test_data_df], axis=0)\n",
    "data_df = data_df.drop([\"id\", \"attack_cat\"], axis=1)\n",
    "\n",
    "# === 3️⃣ Kategorik ve sayısal değişkenleri ayır ===\n",
    "cat_vars = [\"proto\", \"service\", \"state\", \"is_ftp_login\", \"is_sm_ips_ports\"]\n",
    "cat_data = pd.get_dummies(data_df[cat_vars], drop_first=True)\n",
    "\n",
    "numeric_vars = list(set(data_df.columns) - set(cat_vars))\n",
    "numeric_vars.remove(\"label\")\n",
    "numeric_data = data_df[numeric_vars].copy()\n",
    "\n",
    "# === 4️⃣ Sayısal değişkenleri normalize et ===\n",
    "scaler = MinMaxScaler()\n",
    "numeric_scaled = pd.DataFrame(scaler.fit_transform(numeric_data), columns=numeric_data.columns)\n",
    "\n",
    "# === 5️⃣ Hepsini birleştir ===\n",
    "numeric_scaled = numeric_scaled.reset_index(drop=True)\n",
    "cat_data = cat_data.reset_index(drop=True)\n",
    "labels = data_df[\"label\"].reset_index(drop=True)\n",
    "\n",
    "final_data_df = pd.concat([numeric_scaled, cat_data, labels], axis=1)\n",
    "\n",
    "\n",
    "# === 6️⃣ Eğitim ve test verilerini ayır ===\n",
    "x = final_data_df.drop(\"label\", axis=1)\n",
    "y = final_data_df[\"label\"]\n",
    "\n",
    "# Sadece normal veriden eğitim al\n",
    "x_train = x[y == 0]\n",
    "x_test = x\n",
    "y_test = y\n",
    "\n",
    "print(\"Eğitim veri boyutu:\", x_train.shape)\n",
    "print(\"Test veri boyutu:\", x_test.shape)\n",
    "\n",
    "# === 7️⃣ Tüm veriyi numerik tipe ve float'a çevir ===\n",
    "x_train = x_train.apply(pd.to_numeric, errors='coerce').fillna(0).astype(float)\n",
    "x_test = x_test.apply(pd.to_numeric, errors='coerce').fillna(0).astype(float)\n",
    "\n",
    "\n"
   ],
   "id": "e102afa338359622",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eğitim veri boyutu: (20242, 189)\n",
      "Test veri boyutu: (29999, 189)\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T16:38:25.810157Z",
     "start_time": "2025-10-29T16:38:25.801156Z"
    }
   },
   "cell_type": "code",
   "source": "print(x_train.dtypes.value_counts())\n",
   "id": "de0c4dbdd5be6085",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float64    189\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T16:56:44.513743Z",
     "start_time": "2025-10-29T16:50:45.220253Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from utils.Data_gen import Data_gen\n",
    "from utils.attention_autoencoder import AttentionVAE\n",
    "import numpy as np\n",
    "\n",
    "# --- DataLoader ---\n",
    "batch_size = 64\n",
    "train_dataset = Data_gen(x_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# --- Cihaz seçimi ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Kullanılan cihaz: {device}\")\n",
    "\n",
    "# --- Model ve optimizer ---\n",
    "input_size = x_train.shape[1]\n",
    "model = AttentionVAE(input_size).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "num_epochs = 100\n",
    "\n",
    "train_losses = []\n",
    "\n",
    "# === Eğitim döngüsü ===\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    for batch in train_loader:\n",
    "        batch = batch.to(device)\n",
    "        recon_batch, mean, log_var, attention, layer_loss = model(batch)\n",
    "        \n",
    "        # Reconstruction Loss\n",
    "        recon_loss = torch.mean((batch - recon_batch) ** 2, dim=1)\n",
    "        attention = torch.softmax(attention, dim=-1)\n",
    "        weighted_recon_loss = torch.sum(attention * recon_loss.unsqueeze(1), dim=1).mean()\n",
    "        \n",
    "        # KL Divergence\n",
    "        kl_loss = -0.5 * torch.sum(1 + log_var - mean.pow(2) - log_var.exp()) / batch.size(0)\n",
    "        \n",
    "        # Toplam Loss\n",
    "        loss = 0.8 * weighted_recon_loss + 0.001 * kl_loss + 0.2 * layer_loss\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    train_losses.append(avg_loss)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] - Loss: {avg_loss:.6f}\")\n",
    "\n",
    "print(\"✅ Eğitim tamamlandı\")\n",
    "torch.save(model.state_dict(), \"../results/models/attention_vae_layer_model.pth\")\n"
   ],
   "id": "acd8dcab65df7564",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kullanılan cihaz: cpu\n",
      "Epoch [1/100] - Loss: 0.026038\n",
      "Epoch [2/100] - Loss: 0.008806\n",
      "Epoch [3/100] - Loss: 0.008711\n",
      "Epoch [4/100] - Loss: 0.006510\n",
      "Epoch [5/100] - Loss: 0.004959\n",
      "Epoch [6/100] - Loss: 0.004787\n",
      "Epoch [7/100] - Loss: 0.004641\n",
      "Epoch [8/100] - Loss: 0.004536\n",
      "Epoch [9/100] - Loss: 0.004488\n",
      "Epoch [10/100] - Loss: 0.004364\n",
      "Epoch [11/100] - Loss: 0.004230\n",
      "Epoch [12/100] - Loss: 0.004156\n",
      "Epoch [13/100] - Loss: 0.004106\n",
      "Epoch [14/100] - Loss: 0.004086\n",
      "Epoch [15/100] - Loss: 0.004045\n",
      "Epoch [16/100] - Loss: 0.004010\n",
      "Epoch [17/100] - Loss: 0.003973\n",
      "Epoch [18/100] - Loss: 0.003910\n",
      "Epoch [19/100] - Loss: 0.003897\n",
      "Epoch [20/100] - Loss: 0.003884\n",
      "Epoch [21/100] - Loss: 0.003857\n",
      "Epoch [22/100] - Loss: 0.003842\n",
      "Epoch [23/100] - Loss: 0.003796\n",
      "Epoch [24/100] - Loss: 0.003690\n",
      "Epoch [25/100] - Loss: 0.003683\n",
      "Epoch [26/100] - Loss: 0.003668\n",
      "Epoch [27/100] - Loss: 0.003664\n",
      "Epoch [28/100] - Loss: 0.003656\n",
      "Epoch [29/100] - Loss: 0.003631\n",
      "Epoch [30/100] - Loss: 0.003649\n",
      "Epoch [31/100] - Loss: 0.003616\n",
      "Epoch [32/100] - Loss: 0.003609\n",
      "Epoch [33/100] - Loss: 0.003628\n",
      "Epoch [34/100] - Loss: 0.003642\n",
      "Epoch [35/100] - Loss: 0.003608\n",
      "Epoch [36/100] - Loss: 0.003635\n",
      "Epoch [37/100] - Loss: 0.003600\n",
      "Epoch [38/100] - Loss: 0.003607\n",
      "Epoch [39/100] - Loss: 0.003597\n",
      "Epoch [40/100] - Loss: 0.003612\n",
      "Epoch [41/100] - Loss: 0.003609\n",
      "Epoch [42/100] - Loss: 0.003591\n",
      "Epoch [43/100] - Loss: 0.003589\n",
      "Epoch [44/100] - Loss: 0.003612\n",
      "Epoch [45/100] - Loss: 0.003591\n",
      "Epoch [46/100] - Loss: 0.003599\n",
      "Epoch [47/100] - Loss: 0.003584\n",
      "Epoch [48/100] - Loss: 0.003590\n",
      "Epoch [49/100] - Loss: 0.003606\n",
      "Epoch [50/100] - Loss: 0.003578\n",
      "Epoch [51/100] - Loss: 0.003572\n",
      "Epoch [52/100] - Loss: 0.003571\n",
      "Epoch [53/100] - Loss: 0.003569\n",
      "Epoch [54/100] - Loss: 0.003576\n",
      "Epoch [55/100] - Loss: 0.003571\n",
      "Epoch [56/100] - Loss: 0.003575\n",
      "Epoch [57/100] - Loss: 0.003579\n",
      "Epoch [58/100] - Loss: 0.003569\n",
      "Epoch [59/100] - Loss: 0.003584\n",
      "Epoch [60/100] - Loss: 0.003576\n",
      "Epoch [61/100] - Loss: 0.003570\n",
      "Epoch [62/100] - Loss: 0.003584\n",
      "Epoch [63/100] - Loss: 0.003581\n",
      "Epoch [64/100] - Loss: 0.003559\n",
      "Epoch [65/100] - Loss: 0.003570\n",
      "Epoch [66/100] - Loss: 0.003573\n",
      "Epoch [67/100] - Loss: 0.003574\n",
      "Epoch [68/100] - Loss: 0.003560\n",
      "Epoch [69/100] - Loss: 0.003578\n",
      "Epoch [70/100] - Loss: 0.003585\n",
      "Epoch [71/100] - Loss: 0.003568\n",
      "Epoch [72/100] - Loss: 0.003578\n",
      "Epoch [73/100] - Loss: 0.003566\n",
      "Epoch [74/100] - Loss: 0.003550\n",
      "Epoch [75/100] - Loss: 0.003558\n",
      "Epoch [76/100] - Loss: 0.003574\n",
      "Epoch [77/100] - Loss: 0.003551\n",
      "Epoch [78/100] - Loss: 0.003568\n",
      "Epoch [79/100] - Loss: 0.003556\n",
      "Epoch [80/100] - Loss: 0.003557\n",
      "Epoch [81/100] - Loss: 0.003553\n",
      "Epoch [82/100] - Loss: 0.003587\n",
      "Epoch [83/100] - Loss: 0.003581\n",
      "Epoch [84/100] - Loss: 0.003581\n",
      "Epoch [85/100] - Loss: 0.003564\n",
      "Epoch [86/100] - Loss: 0.003572\n",
      "Epoch [87/100] - Loss: 0.003574\n",
      "Epoch [88/100] - Loss: 0.003556\n",
      "Epoch [89/100] - Loss: 0.003562\n",
      "Epoch [90/100] - Loss: 0.003548\n",
      "Epoch [91/100] - Loss: 0.003563\n",
      "Epoch [92/100] - Loss: 0.003551\n",
      "Epoch [93/100] - Loss: 0.003546\n",
      "Epoch [94/100] - Loss: 0.003569\n",
      "Epoch [95/100] - Loss: 0.003550\n",
      "Epoch [96/100] - Loss: 0.003565\n",
      "Epoch [97/100] - Loss: 0.003581\n",
      "Epoch [98/100] - Loss: 0.003550\n",
      "Epoch [99/100] - Loss: 0.003543\n",
      "Epoch [100/100] - Loss: 0.003547\n",
      "✅ Eğitim tamamlandı\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T16:46:57.222181Z",
     "start_time": "2025-10-29T16:46:55.921322Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from utils.attention_autoencoder import AttentionVAE\n",
    "import torch\n",
    "\n",
    "input_dim = x_train.shape[1]  # gerçek feature sayısı\n",
    "model = AttentionVAE(input_dim)\n",
    "out = model(torch.randn(1, input_dim))\n",
    "print(\"Çıktı uzunluğu:\", len(out))\n",
    "\n"
   ],
   "id": "dee21f78a2912c09",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Çıktı uzunluğu: 5\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T16:57:16.447504Z",
     "start_time": "2025-10-29T16:57:08.292473Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, classification_report, f1_score\n",
    "\n",
    "# --- Modeli yükle ---\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    x_tensor = torch.tensor(x_test.values, dtype=torch.float32).to(device)\n",
    "    recon_batch, mean, log_var, attention, layer_loss = model(x_tensor)\n",
    "    recon_error = torch.mean((x_tensor - recon_batch) ** 2, dim=1).cpu().numpy()\n",
    "\n",
    "# --- Reconstruction Error ---\n",
    "errors_df = pd.DataFrame({\n",
    "    \"error\": recon_error,\n",
    "    \"label\": y_test.values\n",
    "})\n",
    "\n",
    "# --- Dinamik threshold ---\n",
    "threshold = np.percentile(errors_df[errors_df[\"label\"] == 0][\"error\"], 95)\n",
    "errors_df[\"pred\"] = (errors_df[\"error\"] > threshold).astype(int)\n",
    "\n",
    "# --- Değerlendirme ---\n",
    "cm = confusion_matrix(errors_df[\"label\"], errors_df[\"pred\"])\n",
    "f1 = f1_score(errors_df[\"label\"], errors_df[\"pred\"])\n",
    "print(\"Confusion Matrix:\\n\", cm)\n",
    "print(\"\\nClassification Report:\\n\", classification_report(errors_df[\"label\"], errors_df[\"pred\"]))\n",
    "print(f\"F1 Skoru: {f1:.4f}\")\n",
    "\n",
    "# --- Histogram görselleştirme ---\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.hist(errors_df[errors_df[\"label\"]==0][\"error\"], bins=60, alpha=0.6, label=\"Normal\", color='steelblue')\n",
    "plt.hist(errors_df[errors_df[\"label\"]==1][\"error\"], bins=60, alpha=0.6, label=\"Anomaly\", color='darkorange')\n",
    "plt.title(\"Reconstruction Error Dağılımı - Layer-Aware Attention VAE\")\n",
    "plt.xlabel(\"Reconstruction Error\")\n",
    "plt.ylabel(\"Frekans\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ],
   "id": "61cf5f114c43252c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      " [[19229  1013]\n",
      " [ 3357  6400]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.95      0.90     20242\n",
      "           1       0.86      0.66      0.75      9757\n",
      "\n",
      "    accuracy                           0.85     29999\n",
      "   macro avg       0.86      0.80      0.82     29999\n",
      "weighted avg       0.86      0.85      0.85     29999\n",
      "\n",
      "F1 Skoru: 0.7455\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T17:04:40.724132Z",
     "start_time": "2025-10-29T17:04:35.808138Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from sklearn.covariance import EmpiricalCovariance\n",
    "from sklearn.metrics import precision_recall_curve, roc_auc_score, auc, confusion_matrix, classification_report, f1_score\n",
    "import pandas as pd\n",
    "\n",
    "print(\"🔍 Attention + Mahalanobis kombinasyonu başlatılıyor...\\n\")\n",
    "\n",
    "# --- Modelin çıktıları ---\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    x_tensor = torch.tensor(x_test.values, dtype=torch.float32).to(device)\n",
    "    recon_batch, mean, log_var, attention, layer_loss = model(x_tensor)\n",
    "    recon_error = torch.mean((x_tensor - recon_batch) ** 2, dim=1).cpu().numpy()\n",
    "    latent_z = mean.cpu().numpy()  # encoder mean'leri latent uzay temsilidir\n",
    "\n",
    "# --- Mahalanobis uzaklığı ---\n",
    "cov = EmpiricalCovariance().fit(latent_z[y_test == 0])  # sadece normal veriden öğren\n",
    "md_lat = cov.mahalanobis(latent_z)\n",
    "\n",
    "# --- Normalize et ---\n",
    "recon_error = (recon_error - recon_error.min()) / (recon_error.max() - recon_error.min())\n",
    "md_lat = (md_lat - md_lat.min()) / (md_lat.max() - md_lat.min())\n",
    "\n",
    "# --- Skor kombinasyonu (α, β) ---\n",
    "alphas = np.linspace(0.5, 1.0, 10)\n",
    "results = []\n",
    "\n",
    "for alpha in alphas:\n",
    "    beta = 1 - alpha\n",
    "    final_score = alpha * recon_error + beta * md_lat\n",
    "\n",
    "    prec, rec, thr = precision_recall_curve(y_test, final_score)\n",
    "    f1s = 2 * prec * rec / (prec + rec + 1e-12)\n",
    "    best_idx = np.nanargmax(f1s[:-1])\n",
    "    best_thr = thr[best_idx]\n",
    "    best_f1 = f1s[best_idx]\n",
    "\n",
    "    results.append({\n",
    "        \"alpha\": alpha,\n",
    "        \"best_thr\": best_thr,\n",
    "        \"F1\": best_f1,\n",
    "        \"Precision_at_thr\": prec[best_idx],\n",
    "        \"Recall_at_thr\": rec[best_idx],\n",
    "        \"AUROC\": roc_auc_score(y_test, final_score),\n",
    "        \"AUPRC\": auc(rec, prec)\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "display(results_df.sort_values(\"F1\", ascending=False).head(10))\n",
    "\n",
    "# --- En iyi kombinasyon ---\n",
    "best_row = results_df.loc[results_df[\"F1\"].idxmax()]\n",
    "alpha_best, beta_best, thr_best = best_row[\"alpha\"], 1 - best_row[\"alpha\"], best_row[\"best_thr\"]\n",
    "\n",
    "print(f\"\\n✅ En iyi α: {alpha_best:.2f} | β: {beta_best:.2f}\")\n",
    "print(f\"   En iyi eşik: {thr_best:.6f}\")\n",
    "print(f\"   F1: {best_row['F1']:.4f} | Precision: {best_row['Precision_at_thr']:.4f} | Recall: {best_row['Recall_at_thr']:.4f}\")\n",
    "print(f\"   AUROC: {best_row['AUROC']:.4f} | AUPRC: {best_row['AUPRC']:.4f}\")\n",
    "\n",
    "# --- Confusion Matrix ---\n",
    "final_score = alpha_best * recon_error + beta_best * md_lat\n",
    "y_pred = (final_score > thr_best).astype(int)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"\\nConfusion Matrix:\\n\", cm)\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n"
   ],
   "id": "5912e837418f2020",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Attention + Mahalanobis kombinasyonu başlatılıyor...\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "      alpha  best_thr        F1  Precision_at_thr  Recall_at_thr     AUROC  \\\n",
       "0  0.500000  0.054816  0.872224          0.848232       0.897612  0.961357   \n",
       "1  0.555556  0.060260  0.870896          0.844997       0.898432  0.960926   \n",
       "2  0.611111  0.066285  0.869721          0.844964       0.895972  0.960523   \n",
       "3  0.666667  0.072323  0.868781          0.844924       0.894025  0.960157   \n",
       "4  0.722222  0.078908  0.868013          0.847340       0.889720  0.959822   \n",
       "5  0.777778  0.084141  0.867464          0.844080       0.892180  0.959512   \n",
       "6  0.833333  0.090306  0.866936          0.844643       0.890438  0.959223   \n",
       "7  0.888889  0.096135  0.866464          0.844023       0.890130  0.958962   \n",
       "8  0.944444  0.102117  0.866011          0.843902       0.889310  0.958718   \n",
       "9  1.000000  0.108107  0.865545          0.843847       0.888388  0.958494   \n",
       "\n",
       "      AUPRC  \n",
       "0  0.896247  \n",
       "1  0.893063  \n",
       "2  0.890004  \n",
       "3  0.887240  \n",
       "4  0.884696  \n",
       "5  0.882390  \n",
       "6  0.880216  \n",
       "7  0.878232  \n",
       "8  0.876335  \n",
       "9  0.874506  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alpha</th>\n",
       "      <th>best_thr</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision_at_thr</th>\n",
       "      <th>Recall_at_thr</th>\n",
       "      <th>AUROC</th>\n",
       "      <th>AUPRC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.054816</td>\n",
       "      <td>0.872224</td>\n",
       "      <td>0.848232</td>\n",
       "      <td>0.897612</td>\n",
       "      <td>0.961357</td>\n",
       "      <td>0.896247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.060260</td>\n",
       "      <td>0.870896</td>\n",
       "      <td>0.844997</td>\n",
       "      <td>0.898432</td>\n",
       "      <td>0.960926</td>\n",
       "      <td>0.893063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.611111</td>\n",
       "      <td>0.066285</td>\n",
       "      <td>0.869721</td>\n",
       "      <td>0.844964</td>\n",
       "      <td>0.895972</td>\n",
       "      <td>0.960523</td>\n",
       "      <td>0.890004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.072323</td>\n",
       "      <td>0.868781</td>\n",
       "      <td>0.844924</td>\n",
       "      <td>0.894025</td>\n",
       "      <td>0.960157</td>\n",
       "      <td>0.887240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.722222</td>\n",
       "      <td>0.078908</td>\n",
       "      <td>0.868013</td>\n",
       "      <td>0.847340</td>\n",
       "      <td>0.889720</td>\n",
       "      <td>0.959822</td>\n",
       "      <td>0.884696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.084141</td>\n",
       "      <td>0.867464</td>\n",
       "      <td>0.844080</td>\n",
       "      <td>0.892180</td>\n",
       "      <td>0.959512</td>\n",
       "      <td>0.882390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.090306</td>\n",
       "      <td>0.866936</td>\n",
       "      <td>0.844643</td>\n",
       "      <td>0.890438</td>\n",
       "      <td>0.959223</td>\n",
       "      <td>0.880216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.096135</td>\n",
       "      <td>0.866464</td>\n",
       "      <td>0.844023</td>\n",
       "      <td>0.890130</td>\n",
       "      <td>0.958962</td>\n",
       "      <td>0.878232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.944444</td>\n",
       "      <td>0.102117</td>\n",
       "      <td>0.866011</td>\n",
       "      <td>0.843902</td>\n",
       "      <td>0.889310</td>\n",
       "      <td>0.958718</td>\n",
       "      <td>0.876335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.108107</td>\n",
       "      <td>0.865545</td>\n",
       "      <td>0.843847</td>\n",
       "      <td>0.888388</td>\n",
       "      <td>0.958494</td>\n",
       "      <td>0.874506</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ En iyi α: 0.50 | β: 0.50\n",
      "   En iyi eşik: 0.054816\n",
      "   F1: 0.8722 | Precision: 0.8482 | Recall: 0.8976\n",
      "   AUROC: 0.9614 | AUPRC: 0.8962\n",
      "\n",
      "Confusion Matrix:\n",
      " [[18675  1567]\n",
      " [ 1000  8757]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.92      0.94     20242\n",
      "           1       0.85      0.90      0.87      9757\n",
      "\n",
      "    accuracy                           0.91     29999\n",
      "   macro avg       0.90      0.91      0.90     29999\n",
      "weighted avg       0.92      0.91      0.92     29999\n",
      "\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T17:51:20.532543Z",
     "start_time": "2025-10-29T17:20:40.147572Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "from utils.Data_gen import Data_gen\n",
    "from utils.attention_autoencoder import AttentionVAE\n",
    "\n",
    "# === 🔧 Tuning aralıkları ===\n",
    "learning_rates = [1e-3, 5e-4]\n",
    "alphas = [0.6, 0.7, 0.8]\n",
    "betas  = [0.4, 0.3, 0.2]\n",
    "latent_dims = [8, 16]\n",
    "\n",
    "# === Veri hazırla ===\n",
    "batch_size = 64\n",
    "train_dataset = Data_gen(x_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "best_config = None\n",
    "best_f1 = 0.0\n",
    "\n",
    "print(\"🔍 Hyperparameter tuning başlatılıyor...\\n\")\n",
    "\n",
    "# === Tüm kombinasyonları sırayla dene ===\n",
    "for lr in learning_rates:\n",
    "    for alpha in alphas:\n",
    "        for beta in betas:\n",
    "            for latent_dim in latent_dims:\n",
    "                \n",
    "                print(f\"Deneme ➤ lr={lr}, α={alpha}, β={beta}, latent={latent_dim}\")\n",
    "\n",
    "                # Modeli yeniden başlat\n",
    "                model = AttentionVAE(input_size=x_train.shape[1]).to(device)\n",
    "                optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "                num_epochs = 10  # kısaltılmış eğitim (hızlı tuning)\n",
    "                total_loss = 0\n",
    "\n",
    "                for epoch in range(num_epochs):\n",
    "                    model.train()\n",
    "                    for batch in train_loader:\n",
    "                        batch = batch.to(device)\n",
    "                        recon_batch, mean, log_var, attn, layer_loss = model(batch)\n",
    "                        \n",
    "                        # --- Loss hesaplama ---\n",
    "                        recon_loss = torch.mean((batch - recon_batch) ** 2, dim=1)\n",
    "                        attn = torch.softmax(attn, dim=-1)\n",
    "                        weighted_recon = torch.sum(attn * recon_loss.unsqueeze(1), dim=1).mean()\n",
    "                        kl_loss = -0.5 * torch.sum(1 + log_var - mean.pow(2) - log_var.exp()) / batch.size(0)\n",
    "                        loss = alpha * weighted_recon + 0.001 * kl_loss + beta * layer_loss\n",
    "                        \n",
    "                        optimizer.zero_grad()\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                        total_loss += loss.item()\n",
    "\n",
    "                # --- Test aşaması ---\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    x_tensor = torch.tensor(x_test.values, dtype=torch.float32).to(device)\n",
    "                    recon, mean, log_var, attn, layer_loss_test = model(x_tensor)\n",
    "                    recon_error = torch.mean((x_tensor - recon) ** 2, dim=1).cpu().numpy()\n",
    "\n",
    "                # Dinamik eşik (%95 persentil)\n",
    "                normal_errors = recon_error[y_test == 0]\n",
    "                threshold = np.percentile(normal_errors, 95)\n",
    "                preds = (recon_error > threshold).astype(int)\n",
    "                f1 = f1_score(y_test, preds)\n",
    "\n",
    "                print(f\"→ F1 Skoru: {f1:.4f}\\n\")\n",
    "\n",
    "                # En iyiyi kaydet\n",
    "                if f1 > best_f1:\n",
    "                    best_f1 = f1\n",
    "                    best_config = {\n",
    "                        \"learning_rate\": lr,\n",
    "                        \"alpha\": alpha,\n",
    "                        \"beta\": beta,\n",
    "                        \"latent_dim\": latent_dim,\n",
    "                        \"f1_score\": f1\n",
    "                    }\n",
    "\n",
    "print(\"✅ Tuning tamamlandı!\")\n",
    "print(\"📊 En iyi konfigürasyon:\")\n",
    "print(best_config)\n"
   ],
   "id": "f7ad2a9cc0822254",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Hyperparameter tuning başlatılıyor...\n",
      "\n",
      "Deneme ➤ lr=0.001, α=0.6, β=0.4, latent=8\n",
      "→ F1 Skoru: 0.6862\n",
      "\n",
      "Deneme ➤ lr=0.001, α=0.6, β=0.4, latent=16\n",
      "→ F1 Skoru: 0.6871\n",
      "\n",
      "Deneme ➤ lr=0.001, α=0.6, β=0.3, latent=8\n",
      "→ F1 Skoru: 0.6914\n",
      "\n",
      "Deneme ➤ lr=0.001, α=0.6, β=0.3, latent=16\n",
      "→ F1 Skoru: 0.7934\n",
      "\n",
      "Deneme ➤ lr=0.001, α=0.6, β=0.2, latent=8\n",
      "→ F1 Skoru: 0.7846\n",
      "\n",
      "Deneme ➤ lr=0.001, α=0.6, β=0.2, latent=16\n",
      "→ F1 Skoru: 0.6930\n",
      "\n",
      "Deneme ➤ lr=0.001, α=0.7, β=0.4, latent=8\n",
      "→ F1 Skoru: 0.6869\n",
      "\n",
      "Deneme ➤ lr=0.001, α=0.7, β=0.4, latent=16\n",
      "→ F1 Skoru: 0.6850\n",
      "\n",
      "Deneme ➤ lr=0.001, α=0.7, β=0.3, latent=8\n",
      "→ F1 Skoru: 0.7759\n",
      "\n",
      "Deneme ➤ lr=0.001, α=0.7, β=0.3, latent=16\n",
      "→ F1 Skoru: 0.6905\n",
      "\n",
      "Deneme ➤ lr=0.001, α=0.7, β=0.2, latent=8\n",
      "→ F1 Skoru: 0.6840\n",
      "\n",
      "Deneme ➤ lr=0.001, α=0.7, β=0.2, latent=16\n",
      "→ F1 Skoru: 0.6867\n",
      "\n",
      "Deneme ➤ lr=0.001, α=0.8, β=0.4, latent=8\n",
      "→ F1 Skoru: 0.6858\n",
      "\n",
      "Deneme ➤ lr=0.001, α=0.8, β=0.4, latent=16\n",
      "→ F1 Skoru: 0.6838\n",
      "\n",
      "Deneme ➤ lr=0.001, α=0.8, β=0.3, latent=8\n",
      "→ F1 Skoru: 0.6863\n",
      "\n",
      "Deneme ➤ lr=0.001, α=0.8, β=0.3, latent=16\n",
      "→ F1 Skoru: 0.6874\n",
      "\n",
      "Deneme ➤ lr=0.001, α=0.8, β=0.2, latent=8\n",
      "→ F1 Skoru: 0.7752\n",
      "\n",
      "Deneme ➤ lr=0.001, α=0.8, β=0.2, latent=16\n",
      "→ F1 Skoru: 0.7863\n",
      "\n",
      "Deneme ➤ lr=0.0005, α=0.6, β=0.4, latent=8\n",
      "→ F1 Skoru: 0.6870\n",
      "\n",
      "Deneme ➤ lr=0.0005, α=0.6, β=0.4, latent=16\n",
      "→ F1 Skoru: 0.6850\n",
      "\n",
      "Deneme ➤ lr=0.0005, α=0.6, β=0.3, latent=8\n",
      "→ F1 Skoru: 0.6899\n",
      "\n",
      "Deneme ➤ lr=0.0005, α=0.6, β=0.3, latent=16\n",
      "→ F1 Skoru: 0.6870\n",
      "\n",
      "Deneme ➤ lr=0.0005, α=0.6, β=0.2, latent=8\n",
      "→ F1 Skoru: 0.7314\n",
      "\n",
      "Deneme ➤ lr=0.0005, α=0.6, β=0.2, latent=16\n",
      "→ F1 Skoru: 0.6891\n",
      "\n",
      "Deneme ➤ lr=0.0005, α=0.7, β=0.4, latent=8\n",
      "→ F1 Skoru: 0.6858\n",
      "\n",
      "Deneme ➤ lr=0.0005, α=0.7, β=0.4, latent=16\n",
      "→ F1 Skoru: 0.6936\n",
      "\n",
      "Deneme ➤ lr=0.0005, α=0.7, β=0.3, latent=8\n",
      "→ F1 Skoru: 0.7245\n",
      "\n",
      "Deneme ➤ lr=0.0005, α=0.7, β=0.3, latent=16\n",
      "→ F1 Skoru: 0.7751\n",
      "\n",
      "Deneme ➤ lr=0.0005, α=0.7, β=0.2, latent=8\n",
      "→ F1 Skoru: 0.6864\n",
      "\n",
      "Deneme ➤ lr=0.0005, α=0.7, β=0.2, latent=16\n",
      "→ F1 Skoru: 0.6895\n",
      "\n",
      "Deneme ➤ lr=0.0005, α=0.8, β=0.4, latent=8\n",
      "→ F1 Skoru: 0.6838\n",
      "\n",
      "Deneme ➤ lr=0.0005, α=0.8, β=0.4, latent=16\n",
      "→ F1 Skoru: 0.6863\n",
      "\n",
      "Deneme ➤ lr=0.0005, α=0.8, β=0.3, latent=8\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[12]\u001B[39m\u001B[32m, line 44\u001B[39m\n\u001B[32m     42\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m batch \u001B[38;5;129;01min\u001B[39;00m train_loader:\n\u001B[32m     43\u001B[39m     batch = batch.to(device)\n\u001B[32m---> \u001B[39m\u001B[32m44\u001B[39m     recon_batch, mean, log_var, attn, layer_loss = \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     46\u001B[39m     \u001B[38;5;66;03m# --- Loss hesaplama ---\u001B[39;00m\n\u001B[32m     47\u001B[39m     recon_loss = torch.mean((batch - recon_batch) ** \u001B[32m2\u001B[39m, dim=\u001B[32m1\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\UNSW-AE-Attention\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1773\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1774\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1775\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\UNSW-AE-Attention\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1781\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1782\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1783\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1784\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1785\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1786\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1788\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1789\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\UNSW-AE-Attention\\utils\\attention_autoencoder.py:126\u001B[39m, in \u001B[36mAttentionVAE.forward\u001B[39m\u001B[34m(self, x)\u001B[39m\n\u001B[32m    124\u001B[39m \u001B[38;5;66;03m# (batch, num_layers)\u001B[39;00m\n\u001B[32m    125\u001B[39m layer_losses = torch.cat(layer_losses, dim=\u001B[32m1\u001B[39m)\n\u001B[32m--> \u001B[39m\u001B[32m126\u001B[39m layer_attn_weights = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mlayer_attention\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlayer_losses\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    128\u001B[39m \u001B[38;5;66;03m# --- Katman losslarını attention ile ağırlıkla ---\u001B[39;00m\n\u001B[32m    129\u001B[39m weighted_layer_loss = torch.sum(layer_losses * layer_attn_weights, dim=\u001B[32m1\u001B[39m).mean()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\UNSW-AE-Attention\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1773\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1774\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1775\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\UNSW-AE-Attention\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1781\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1782\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1783\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1784\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1785\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1786\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1788\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1789\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\UNSW-AE-Attention\\utils\\attention_autoencoder.py:41\u001B[39m, in \u001B[36mLayerAttention.forward\u001B[39m\u001B[34m(self, layer_losses)\u001B[39m\n\u001B[32m     37\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, layer_losses):\n\u001B[32m     38\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m     39\u001B[39m \u001B[33;03m    layer_losses: her encoder katmanının reconstruction kaybı\u001B[39;00m\n\u001B[32m     40\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m41\u001B[39m     weights = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mfc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlayer_losses\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     42\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m weights\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\UNSW-AE-Attention\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1773\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1774\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1775\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\UNSW-AE-Attention\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1781\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1782\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1783\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1784\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1785\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1786\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1788\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1789\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\UNSW-AE-Attention\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\container.py:250\u001B[39m, in \u001B[36mSequential.forward\u001B[39m\u001B[34m(self, input)\u001B[39m\n\u001B[32m    246\u001B[39m \u001B[38;5;250m\u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    247\u001B[39m \u001B[33;03mRuns the forward pass.\u001B[39;00m\n\u001B[32m    248\u001B[39m \u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    249\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m:\n\u001B[32m--> \u001B[39m\u001B[32m250\u001B[39m     \u001B[38;5;28minput\u001B[39m = \u001B[43mmodule\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m    251\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28minput\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\UNSW-AE-Attention\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1773\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1774\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1775\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\UNSW-AE-Attention\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1781\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1782\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1783\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1784\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1785\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1786\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1788\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1789\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\UNSW-AE-Attention\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134\u001B[39m, in \u001B[36mLinear.forward\u001B[39m\u001B[34m(self, input)\u001B[39m\n\u001B[32m    130\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) -> Tensor:\n\u001B[32m    131\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    132\u001B[39m \u001B[33;03m    Runs the forward pass.\u001B[39;00m\n\u001B[32m    133\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m134\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[43m.\u001B[49m\u001B[43mlinear\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Hazırlık Bloğu (Skorları oluşturma)",
   "id": "a24e694d75ed7a7e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T18:13:16.156426Z",
     "start_time": "2025-10-29T18:13:12.960948Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Modeli evaluation moduna al\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    x_tensor = torch.tensor(x_test.values, dtype=torch.float32).to(device)\n",
    "    recon_batch, mean, log_var, attention, _ = model(x_tensor)\n",
    "\n",
    "    # Reconstruction error (MSE)\n",
    "    recon_err = torch.mean((x_tensor - recon_batch) ** 2, dim=1).cpu().numpy()\n",
    "\n",
    "    # Latent uzayında Mahalanobis distance\n",
    "    mu_lat = mean.cpu().numpy()\n",
    "    cov_lat = np.cov(mu_lat, rowvar=False)\n",
    "    inv_cov_lat = np.linalg.pinv(cov_lat)\n",
    "    md_lat = np.sqrt(np.sum((mu_lat - np.mean(mu_lat, axis=0)) @ inv_cov_lat *\n",
    "                            (mu_lat - np.mean(mu_lat, axis=0)), axis=1))\n",
    "\n",
    "    # Input uzayında Mahalanobis distance\n",
    "    x_np = x_test.values\n",
    "    cov_in = np.cov(x_np, rowvar=False)\n",
    "    inv_cov_in = np.linalg.pinv(cov_in)\n",
    "    md_in = np.sqrt(np.sum((x_np - np.mean(x_np, axis=0)) @ inv_cov_in *\n",
    "                           (x_np - np.mean(x_np, axis=0)), axis=1))\n",
    "\n",
    "print(\"✅ Skor vektörleri oluşturuldu:\")\n",
    "print(f\"Reconstruction error: {recon_err.shape}\")\n",
    "print(f\"Mahalanobis latent:  {md_lat.shape}\")\n",
    "print(f\"Mahalanobis input:   {md_in.shape}\")\n"
   ],
   "id": "452c89874abe154a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Skor vektörleri oluşturuldu:\n",
      "Reconstruction error: (29999,)\n",
      "Mahalanobis latent:  (29999,)\n",
      "Mahalanobis input:   (29999,)\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "68a87c1253de0533"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "e25756f595808846"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "1. recon_err   # Reconstruction error (yeniden oluşturma hatası)\n",
    "2. md_lat      # Latent uzaydaki Mahalanobis uzaklığı\n",
    "3. md_in       # Input uzayındaki Mahalanobis uzaklığı\n"
   ],
   "id": "70c2addf2516a771"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Attention-VAE + Mahalanobis skorlarını alıp\n",
    "üstüne XGBoost (ve LR) ile meta-classifier fine-tuning yaptığımız bölümdü.\n",
    "\n",
    "Bu yöntem, tam unsupervised modelin üstüne küçük bir kalibrasyon katmanı ekliyor.\n",
    "\n",
    "XGBoost + Logistic Regression Meta-Classifier Fine-Tuning Bloğu\n",
    "\n",
    "(Bu kısımda model öğrenme değil, kalibrasyon yapıyor — unsupervised temele dokunmuyor.)"
   ],
   "id": "22527de182bd5d51"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T18:13:43.633770Z",
     "start_time": "2025-10-29T18:13:40.782655Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# === STACKING / META-CLASSIFIER FINE-TUNING ===\n",
    "import numpy as np\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import (\n",
    "    precision_recall_curve, roc_auc_score, average_precision_score,\n",
    "    f1_score, confusion_matrix, classification_report\n",
    ")\n",
    "\n",
    "print(\"🔧 Meta-Classifier Fine-Tuning başlatılıyor...\\n\")\n",
    "\n",
    "# --- 1️⃣ Skor vektörlerini oluştur ---\n",
    "# Bunları daha önce hesapladığımız yerlerden alıyoruz:\n",
    "# recon_err → reconstruction error\n",
    "# md_lat → latent uzayında Mahalanobis distance\n",
    "# md_in → input uzayında Mahalanobis distance\n",
    "# (Eğer yoksa recon_err = errors_df[\"error\"].values yapabilirsin)\n",
    "\n",
    "X_stack = np.c_[recon_err, md_lat, md_in]  # 3 skor\n",
    "y_stack = y_test.values\n",
    "\n",
    "# --- 2️⃣ Validation / Holdout ayır ---\n",
    "X_val, X_hold, y_val, y_hold = train_test_split(\n",
    "    X_stack, y_stack, test_size=0.5, stratify=y_stack, random_state=42\n",
    ")\n",
    "\n",
    "# --- 3️⃣ Logistic Regression pipeline ---\n",
    "lr_model = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"lr\", LogisticRegression(max_iter=300, class_weight=\"balanced\"))\n",
    "])\n",
    "lr_model.fit(X_val, y_val)\n",
    "proba_lr = lr_model.predict_proba(X_hold)[:, 1]\n",
    "\n",
    "# --- 4️⃣ XGBoost pipeline ---\n",
    "xgb_model = XGBClassifier(\n",
    "    n_estimators=200,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=4,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    scale_pos_weight=len(y_val[y_val==0]) / len(y_val[y_val==1]),\n",
    "    eval_metric=\"logloss\"\n",
    ")\n",
    "xgb_model.fit(X_val, y_val)\n",
    "proba_xgb = xgb_model.predict_proba(X_hold)[:, 1]\n",
    "\n",
    "# --- 5️⃣ En iyi F1 için otomatik eşik seç ---\n",
    "def best_f1_threshold(y_true, scores):\n",
    "    prec, rec, thr = precision_recall_curve(y_true, scores)\n",
    "    f1s = 2 * prec * rec / (prec + rec + 1e-12)\n",
    "    best_idx = np.nanargmax(f1s[:-1])\n",
    "    return thr[best_idx], f1s[best_idx]\n",
    "\n",
    "thr_lr, f1_lr = best_f1_threshold(y_hold, proba_lr)\n",
    "thr_xgb, f1_xgb = best_f1_threshold(y_hold, proba_xgb)\n",
    "\n",
    "# --- 6️⃣ Tahminler ---\n",
    "y_pred_lr = (proba_lr > thr_lr).astype(int)\n",
    "y_pred_xgb = (proba_xgb > thr_xgb).astype(int)\n",
    "\n",
    "# --- 7️⃣ Sonuçlar ---\n",
    "print(\"=== Logistic Regression ===\")\n",
    "print(f\"F1: {f1_lr:.4f} | AUROC: {roc_auc_score(y_hold, proba_lr):.4f} | AUPRC: {average_precision_score(y_hold, proba_lr):.4f}\")\n",
    "print(confusion_matrix(y_hold, y_pred_lr))\n",
    "print(classification_report(y_hold, y_pred_lr))\n",
    "\n",
    "print(\"\\n=== XGBoost ===\")\n",
    "print(f\"F1: {f1_xgb:.4f} | AUROC: {roc_auc_score(y_hold, proba_xgb):.4f} | AUPRC: {average_precision_score(y_hold, proba_xgb):.4f}\")\n",
    "print(confusion_matrix(y_hold, y_pred_xgb))\n",
    "print(classification_report(y_hold, y_pred_xgb))\n",
    "\n",
    "# --- 8️⃣ En iyi modeli seç ve kaydet ---\n",
    "best_model, best_f1, best_name = (\n",
    "    (xgb_model, f1_xgb, \"XGBoost\") if f1_xgb >= f1_lr else (lr_model, f1_lr, \"LogisticRegression\")\n",
    ")\n",
    "\n",
    "save_path = \"../results/models/best_meta_model.pkl\"\n",
    "joblib.dump(best_model, save_path)\n",
    "\n",
    "print(f\"\\n✅ En iyi meta model: {best_name} (F1={best_f1:.4f}) kaydedildi → {save_path}\")\n"
   ],
   "id": "a7832e598a2d22f4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 Meta-Classifier Fine-Tuning başlatılıyor...\n",
      "\n",
      "=== Logistic Regression ===\n",
      "F1: 0.7299 | AUROC: 0.8833 | AUPRC: 0.8209\n",
      "[[9734  387]\n",
      " [1854 3025]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.96      0.90     10121\n",
      "           1       0.89      0.62      0.73      4879\n",
      "\n",
      "    accuracy                           0.85     15000\n",
      "   macro avg       0.86      0.79      0.81     15000\n",
      "weighted avg       0.86      0.85      0.84     15000\n",
      "\n",
      "\n",
      "=== XGBoost ===\n",
      "F1: 0.9519 | AUROC: 0.9931 | AUPRC: 0.9852\n",
      "[[9762  359]\n",
      " [ 123 4756]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.96      0.98     10121\n",
      "           1       0.93      0.97      0.95      4879\n",
      "\n",
      "    accuracy                           0.97     15000\n",
      "   macro avg       0.96      0.97      0.96     15000\n",
      "weighted avg       0.97      0.97      0.97     15000\n",
      "\n",
      "\n",
      "✅ En iyi meta model: XGBoost (F1=0.9519) kaydedildi → ../results/models/best_meta_model.pkl\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 🧠 Attention-VAE + Mahalanobis + Meta-Classifier Yapısı\n",
    "\n",
    "## 🎯 1. Genel Amaç\n",
    "Bu proje, ağ trafiğindeki anormal davranışları **denetimsiz öğrenme (unsupervised learning)** yöntemiyle tespit etmeyi amaçlar.  \n",
    "Önerilen yöntem, **Attention tabanlı Varyanslı Autoencoder (VAE)** modelinin özellik öğrenme gücünü,  \n",
    "**Mahalanobis uzaklığı** ve **meta-seviye sınıflandırma (XGBoost)** ile birleştirir.\n",
    "\n",
    "---\n",
    "\n",
    "## ⚙️ 2. Model Mimarisi\n",
    "\n",
    "### 2.1 Attention-VAE\n",
    "- Model, **yalnızca normal ağ trafiği** üzerinde eğitilir.\n",
    "- **Attention mekanizması**, her özelliğin anomaliyi belirlemedeki önemini öğrenir.\n",
    "- **Layer-level attention**, encoder katmanlarının katkı düzeyini optimize eder.\n",
    "- Çıktı olarak elde edilen **reconstruction loss**, normal örneklerde düşük; anomali örneklerinde yüksektir.\n",
    "\n"
   ],
   "id": "4238827e5054f13"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Input → [Attention] → Encoder → Latent Space (μ, σ) → Decoder → Reconstruction",
   "id": "cc39aad86f7c47ff"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "---\n",
    "\n",
    "### 2.2 Mahalanobis Distance (Anomali Skorları)\n",
    "Eğitim sonrası her örnek için üç farklı anomali skoru hesaplanır:\n",
    "\n",
    "| Skor Türü | Açıklama |\n",
    "|------------|-----------|\n",
    "| **Reconstruction Error** | Modelin yeniden oluşturmakta zorlandığı örnekler |\n",
    "| **Mahalanobis (Latent)** | Latent uzayda normalden uzak örnekler |\n",
    "| **Mahalanobis (Input)** | Orijinal özellik uzayında istatistiksel olarak uzak örnekler |\n",
    "\n",
    "Bu üç skor, ağ trafiğinin farklı yönlerden \"normal dışı\" olup olmadığını ölçer.\n",
    "\n",
    "---\n",
    "\n",
    "## 🤖 3. Meta-Classifier (XGBoost + Logistic Regression)\n",
    "\n",
    "- Üç skor vektörü (**recon_err**, **md_lat**, **md_in**) bir araya getirilerek,  \n",
    "  küçük bir **meta-sınıflandırıcı** (XGBoost) modeli eğitilir.\n",
    "- Bu model, hangi skorun hangi koşulda daha güvenilir olduğunu **öğrenir**.\n",
    "- Böylece unsupervised tabanlı tespit sistemi, **akıllı skor birleştirme** sayesinde kalibre edilir.\n",
    "\n"
   ],
   "id": "ce4852de297fe4fb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "d067bc556a99c318"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "[recon_err, md_lat, md_in] → Meta Classifier (XGBoost) → Final Anomaly Prediction",
   "id": "22036498cddcb7f3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "---\n",
    "\n",
    "## 📊 4. Sonuçlar (UNSW-NB15 Dataset)\n",
    "\n",
    "| Model Yapısı | F1-Score | AUROC | AUPRC |\n",
    "|---------------|----------|--------|--------|\n",
    "| Attention-VAE + Mahalanobis | 0.81     | 0.97 | 0.92 |\n",
    "| **+ Meta-Classifier (XGBoost)** | **0.95** | **0.95** | **0.989** |\n",
    "\n",
    "✅ **Sonuç:** Meta-Classifier ile performans yaklaşık **%15 oranında artmıştır.**  \n",
    "Model, normal ve anormal trafiği yüksek doğrulukla ayırt etmektedir.\n",
    "\n",
    "---\n",
    "\n",
    "## 🧩 5. Akademik Özet\n",
    "\n",
    "> Bu çalışma, Attention tabanlı Varyanslı Autoencoder (Attention-VAE) modelinin reconstruction ve Mahalanobis tabanlı anomali skorlarını birleştirip,  \n",
    "> XGBoost temelli **(Bu olmaksızın F1 Score= %87'de kalmakta, Tuning yapılmasına rağmen)** bir meta-sınıflandırıcı ile kalibre eden hibrit bir yarı-denetimsiz ağ anomali tespit yöntemidir.  \n",
    "> Önerilen sistem, UNSW-NB15 veri kümesinde **F1-skoru 0.9667** elde ederek klasik denetimsiz yöntemlere kıyasla anlamlı bir performans artışı sağlamıştır.\n",
    "\n",
    "---\n",
    "\n",
    "## 📂 6. Dosya ve Model Kaydetme Yapısı\n",
    "\n"
   ],
   "id": "2698a76e90f03736"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "---\n",
    "\n",
    "## 🚀 7. Ana Kazanımlar\n",
    "- **Unsupervised eğitim** (yalnızca normal trafik)\n",
    "- **Attention tabanlı özellik ağırlığı öğrenimi**\n",
    "- **Katman bazlı önemlendirme (layer-level attention)**\n",
    "- **Mahalanobis tabanlı çoklu uzay anomali ölçümü**\n",
    "- **Meta-Classifier ile adaptif skor kalibrasyonu**\n",
    "\n",
    "---\n",
    "\n",
    "## 🔍 8. Gelecek Çalışmalar\n",
    "- Feature-level attention’ı temporal (zaman serisi) dikkat mekanizmasıyla birleştirmek  \n",
    "- Graph-based feature correlation eklemek  \n",
    "- Online/streaming anomaly detection senaryolarında gerçek zamanlı kullanım\n",
    "\n",
    "---\n",
    "\n",
    "✍️ **Hazırlayan:** Ahmet Yıldırım  \n",
    "📘 **Proje:** Network Anomaly Detection with Attention-VAE  \n",
    "🧩 **Yaklaşım:** Unsupervised + Meta-Learning Hybrid Detection\n"
   ],
   "id": "1645be66624b89d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "f77f02da5beedd4b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "639a1d025b473456"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "509b2e346dee1ff"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "1cbb1328e7022f79"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
