{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T16:38:22.991668Z",
     "start_time": "2025-10-29T16:38:19.403097Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# === 1Ô∏è‚É£ Veri y√ºkleme ===\n",
    "train_data_df = pd.read_csv('../data/UNSW_NB15_training-set.csv')\n",
    "test_data_df = pd.read_csv('../data/UNSW_NB15_testing-set.csv')\n",
    "\n",
    "\n",
    "# === 2Ô∏è‚É£ Verileri birle≈ütir ===\n",
    "data_df = pd.concat([train_data_df, test_data_df], axis=0)\n",
    "data_df = data_df.drop([\"id\", \"attack_cat\"], axis=1)\n",
    "\n",
    "# === 3Ô∏è‚É£ Kategorik ve sayƒ±sal deƒüi≈ükenleri ayƒ±r ===\n",
    "cat_vars = [\"proto\", \"service\", \"state\", \"is_ftp_login\", \"is_sm_ips_ports\"]\n",
    "cat_data = pd.get_dummies(data_df[cat_vars], drop_first=True)\n",
    "\n",
    "numeric_vars = list(set(data_df.columns) - set(cat_vars))\n",
    "numeric_vars.remove(\"label\")\n",
    "numeric_data = data_df[numeric_vars].copy()\n",
    "\n",
    "# === 4Ô∏è‚É£ Sayƒ±sal deƒüi≈ükenleri normalize et ===\n",
    "scaler = MinMaxScaler()\n",
    "numeric_scaled = pd.DataFrame(scaler.fit_transform(numeric_data), columns=numeric_data.columns)\n",
    "\n",
    "# === 5Ô∏è‚É£ Hepsini birle≈ütir ===\n",
    "numeric_scaled = numeric_scaled.reset_index(drop=True)\n",
    "cat_data = cat_data.reset_index(drop=True)\n",
    "labels = data_df[\"label\"].reset_index(drop=True)\n",
    "\n",
    "final_data_df = pd.concat([numeric_scaled, cat_data, labels], axis=1)\n",
    "\n",
    "\n",
    "# === 6Ô∏è‚É£ Eƒüitim ve test verilerini ayƒ±r ===\n",
    "x = final_data_df.drop(\"label\", axis=1)\n",
    "y = final_data_df[\"label\"]\n",
    "\n",
    "# Sadece normal veriden eƒüitim al\n",
    "x_train = x[y == 0]\n",
    "x_test = x\n",
    "y_test = y\n",
    "\n",
    "print(\"Eƒüitim veri boyutu:\", x_train.shape)\n",
    "print(\"Test veri boyutu:\", x_test.shape)\n",
    "\n",
    "# === 7Ô∏è‚É£ T√ºm veriyi numerik tipe ve float'a √ßevir ===\n",
    "x_train = x_train.apply(pd.to_numeric, errors='coerce').fillna(0).astype(float)\n",
    "x_test = x_test.apply(pd.to_numeric, errors='coerce').fillna(0).astype(float)\n",
    "\n",
    "\n"
   ],
   "id": "e102afa338359622",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eƒüitim veri boyutu: (20242, 189)\n",
      "Test veri boyutu: (29999, 189)\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T16:38:25.810157Z",
     "start_time": "2025-10-29T16:38:25.801156Z"
    }
   },
   "cell_type": "code",
   "source": "print(x_train.dtypes.value_counts())\n",
   "id": "de0c4dbdd5be6085",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float64    189\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T16:56:44.513743Z",
     "start_time": "2025-10-29T16:50:45.220253Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from utils.Data_gen import Data_gen\n",
    "from utils.attention_autoencoder import AttentionVAE\n",
    "import numpy as np\n",
    "\n",
    "# --- DataLoader ---\n",
    "batch_size = 64\n",
    "train_dataset = Data_gen(x_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# --- Cihaz se√ßimi ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Kullanƒ±lan cihaz: {device}\")\n",
    "\n",
    "# --- Model ve optimizer ---\n",
    "input_size = x_train.shape[1]\n",
    "model = AttentionVAE(input_size).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "num_epochs = 100\n",
    "\n",
    "train_losses = []\n",
    "\n",
    "# === Eƒüitim d√∂ng√ºs√º ===\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    for batch in train_loader:\n",
    "        batch = batch.to(device)\n",
    "        recon_batch, mean, log_var, attention, layer_loss = model(batch)\n",
    "        \n",
    "        # Reconstruction Loss\n",
    "        recon_loss = torch.mean((batch - recon_batch) ** 2, dim=1)\n",
    "        attention = torch.softmax(attention, dim=-1)\n",
    "        weighted_recon_loss = torch.sum(attention * recon_loss.unsqueeze(1), dim=1).mean()\n",
    "        \n",
    "        # KL Divergence\n",
    "        kl_loss = -0.5 * torch.sum(1 + log_var - mean.pow(2) - log_var.exp()) / batch.size(0)\n",
    "        \n",
    "        # Toplam Loss\n",
    "        loss = 0.8 * weighted_recon_loss + 0.001 * kl_loss + 0.2 * layer_loss\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    train_losses.append(avg_loss)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] - Loss: {avg_loss:.6f}\")\n",
    "\n",
    "print(\"‚úÖ Eƒüitim tamamlandƒ±\")\n",
    "torch.save(model.state_dict(), \"../results/models/attention_vae_layer_model.pth\")\n"
   ],
   "id": "acd8dcab65df7564",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kullanƒ±lan cihaz: cpu\n",
      "Epoch [1/100] - Loss: 0.026038\n",
      "Epoch [2/100] - Loss: 0.008806\n",
      "Epoch [3/100] - Loss: 0.008711\n",
      "Epoch [4/100] - Loss: 0.006510\n",
      "Epoch [5/100] - Loss: 0.004959\n",
      "Epoch [6/100] - Loss: 0.004787\n",
      "Epoch [7/100] - Loss: 0.004641\n",
      "Epoch [8/100] - Loss: 0.004536\n",
      "Epoch [9/100] - Loss: 0.004488\n",
      "Epoch [10/100] - Loss: 0.004364\n",
      "Epoch [11/100] - Loss: 0.004230\n",
      "Epoch [12/100] - Loss: 0.004156\n",
      "Epoch [13/100] - Loss: 0.004106\n",
      "Epoch [14/100] - Loss: 0.004086\n",
      "Epoch [15/100] - Loss: 0.004045\n",
      "Epoch [16/100] - Loss: 0.004010\n",
      "Epoch [17/100] - Loss: 0.003973\n",
      "Epoch [18/100] - Loss: 0.003910\n",
      "Epoch [19/100] - Loss: 0.003897\n",
      "Epoch [20/100] - Loss: 0.003884\n",
      "Epoch [21/100] - Loss: 0.003857\n",
      "Epoch [22/100] - Loss: 0.003842\n",
      "Epoch [23/100] - Loss: 0.003796\n",
      "Epoch [24/100] - Loss: 0.003690\n",
      "Epoch [25/100] - Loss: 0.003683\n",
      "Epoch [26/100] - Loss: 0.003668\n",
      "Epoch [27/100] - Loss: 0.003664\n",
      "Epoch [28/100] - Loss: 0.003656\n",
      "Epoch [29/100] - Loss: 0.003631\n",
      "Epoch [30/100] - Loss: 0.003649\n",
      "Epoch [31/100] - Loss: 0.003616\n",
      "Epoch [32/100] - Loss: 0.003609\n",
      "Epoch [33/100] - Loss: 0.003628\n",
      "Epoch [34/100] - Loss: 0.003642\n",
      "Epoch [35/100] - Loss: 0.003608\n",
      "Epoch [36/100] - Loss: 0.003635\n",
      "Epoch [37/100] - Loss: 0.003600\n",
      "Epoch [38/100] - Loss: 0.003607\n",
      "Epoch [39/100] - Loss: 0.003597\n",
      "Epoch [40/100] - Loss: 0.003612\n",
      "Epoch [41/100] - Loss: 0.003609\n",
      "Epoch [42/100] - Loss: 0.003591\n",
      "Epoch [43/100] - Loss: 0.003589\n",
      "Epoch [44/100] - Loss: 0.003612\n",
      "Epoch [45/100] - Loss: 0.003591\n",
      "Epoch [46/100] - Loss: 0.003599\n",
      "Epoch [47/100] - Loss: 0.003584\n",
      "Epoch [48/100] - Loss: 0.003590\n",
      "Epoch [49/100] - Loss: 0.003606\n",
      "Epoch [50/100] - Loss: 0.003578\n",
      "Epoch [51/100] - Loss: 0.003572\n",
      "Epoch [52/100] - Loss: 0.003571\n",
      "Epoch [53/100] - Loss: 0.003569\n",
      "Epoch [54/100] - Loss: 0.003576\n",
      "Epoch [55/100] - Loss: 0.003571\n",
      "Epoch [56/100] - Loss: 0.003575\n",
      "Epoch [57/100] - Loss: 0.003579\n",
      "Epoch [58/100] - Loss: 0.003569\n",
      "Epoch [59/100] - Loss: 0.003584\n",
      "Epoch [60/100] - Loss: 0.003576\n",
      "Epoch [61/100] - Loss: 0.003570\n",
      "Epoch [62/100] - Loss: 0.003584\n",
      "Epoch [63/100] - Loss: 0.003581\n",
      "Epoch [64/100] - Loss: 0.003559\n",
      "Epoch [65/100] - Loss: 0.003570\n",
      "Epoch [66/100] - Loss: 0.003573\n",
      "Epoch [67/100] - Loss: 0.003574\n",
      "Epoch [68/100] - Loss: 0.003560\n",
      "Epoch [69/100] - Loss: 0.003578\n",
      "Epoch [70/100] - Loss: 0.003585\n",
      "Epoch [71/100] - Loss: 0.003568\n",
      "Epoch [72/100] - Loss: 0.003578\n",
      "Epoch [73/100] - Loss: 0.003566\n",
      "Epoch [74/100] - Loss: 0.003550\n",
      "Epoch [75/100] - Loss: 0.003558\n",
      "Epoch [76/100] - Loss: 0.003574\n",
      "Epoch [77/100] - Loss: 0.003551\n",
      "Epoch [78/100] - Loss: 0.003568\n",
      "Epoch [79/100] - Loss: 0.003556\n",
      "Epoch [80/100] - Loss: 0.003557\n",
      "Epoch [81/100] - Loss: 0.003553\n",
      "Epoch [82/100] - Loss: 0.003587\n",
      "Epoch [83/100] - Loss: 0.003581\n",
      "Epoch [84/100] - Loss: 0.003581\n",
      "Epoch [85/100] - Loss: 0.003564\n",
      "Epoch [86/100] - Loss: 0.003572\n",
      "Epoch [87/100] - Loss: 0.003574\n",
      "Epoch [88/100] - Loss: 0.003556\n",
      "Epoch [89/100] - Loss: 0.003562\n",
      "Epoch [90/100] - Loss: 0.003548\n",
      "Epoch [91/100] - Loss: 0.003563\n",
      "Epoch [92/100] - Loss: 0.003551\n",
      "Epoch [93/100] - Loss: 0.003546\n",
      "Epoch [94/100] - Loss: 0.003569\n",
      "Epoch [95/100] - Loss: 0.003550\n",
      "Epoch [96/100] - Loss: 0.003565\n",
      "Epoch [97/100] - Loss: 0.003581\n",
      "Epoch [98/100] - Loss: 0.003550\n",
      "Epoch [99/100] - Loss: 0.003543\n",
      "Epoch [100/100] - Loss: 0.003547\n",
      "‚úÖ Eƒüitim tamamlandƒ±\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T16:46:57.222181Z",
     "start_time": "2025-10-29T16:46:55.921322Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from utils.attention_autoencoder import AttentionVAE\n",
    "import torch\n",
    "\n",
    "input_dim = x_train.shape[1]  # ger√ßek feature sayƒ±sƒ±\n",
    "model = AttentionVAE(input_dim)\n",
    "out = model(torch.randn(1, input_dim))\n",
    "print(\"√áƒ±ktƒ± uzunluƒüu:\", len(out))\n",
    "\n"
   ],
   "id": "dee21f78a2912c09",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "√áƒ±ktƒ± uzunluƒüu: 5\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T16:57:16.447504Z",
     "start_time": "2025-10-29T16:57:08.292473Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, classification_report, f1_score\n",
    "\n",
    "# --- Modeli y√ºkle ---\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    x_tensor = torch.tensor(x_test.values, dtype=torch.float32).to(device)\n",
    "    recon_batch, mean, log_var, attention, layer_loss = model(x_tensor)\n",
    "    recon_error = torch.mean((x_tensor - recon_batch) ** 2, dim=1).cpu().numpy()\n",
    "\n",
    "# --- Reconstruction Error ---\n",
    "errors_df = pd.DataFrame({\n",
    "    \"error\": recon_error,\n",
    "    \"label\": y_test.values\n",
    "})\n",
    "\n",
    "# --- Dinamik threshold ---\n",
    "threshold = np.percentile(errors_df[errors_df[\"label\"] == 0][\"error\"], 95)\n",
    "errors_df[\"pred\"] = (errors_df[\"error\"] > threshold).astype(int)\n",
    "\n",
    "# --- Deƒüerlendirme ---\n",
    "cm = confusion_matrix(errors_df[\"label\"], errors_df[\"pred\"])\n",
    "f1 = f1_score(errors_df[\"label\"], errors_df[\"pred\"])\n",
    "print(\"Confusion Matrix:\\n\", cm)\n",
    "print(\"\\nClassification Report:\\n\", classification_report(errors_df[\"label\"], errors_df[\"pred\"]))\n",
    "print(f\"F1 Skoru: {f1:.4f}\")\n",
    "\n",
    "# --- Histogram g√∂rselle≈ütirme ---\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.hist(errors_df[errors_df[\"label\"]==0][\"error\"], bins=60, alpha=0.6, label=\"Normal\", color='steelblue')\n",
    "plt.hist(errors_df[errors_df[\"label\"]==1][\"error\"], bins=60, alpha=0.6, label=\"Anomaly\", color='darkorange')\n",
    "plt.title(\"Reconstruction Error Daƒüƒ±lƒ±mƒ± - Layer-Aware Attention VAE\")\n",
    "plt.xlabel(\"Reconstruction Error\")\n",
    "plt.ylabel(\"Frekans\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ],
   "id": "61cf5f114c43252c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      " [[19229  1013]\n",
      " [ 3357  6400]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.95      0.90     20242\n",
      "           1       0.86      0.66      0.75      9757\n",
      "\n",
      "    accuracy                           0.85     29999\n",
      "   macro avg       0.86      0.80      0.82     29999\n",
      "weighted avg       0.86      0.85      0.85     29999\n",
      "\n",
      "F1 Skoru: 0.7455\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ],
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAsAAAAHWCAYAAAB5SD/0AAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjcsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvTLEjVAAAAAlwSFlzAAAPYQAAD2EBqD+naQAATy1JREFUeJzt3Qm8TPUf//GPfc2+Zy27LFmSNolQCqWFCkULUZailBItikoq0SLaQ2mjrKFCKZElRCn62RWyb+f/eH///zP/mXEv915z1/N6Ph7DnTNnzpxz5syZz3zO53xOJs/zPAMAAAACInNqzwAAAACQkgiAAQAAECgEwAAAAAgUAmAAAAAECgEwAAAAAoUAGAAAAIFCAAwAAIBAIQAGAABAoBAAAwAAIFAIgIEMZO7cuZYpUyb3P9KG+vXr21tvvWWHDx+2JUuWWP78+W3v3r2nfF758uXt1ltvTZF5BDICfV70uQESggAYp238+PEu6PJvWbNmtTPPPNPtjP73v/9luDX8yiuvuGUO+jxEu/TSSyO2g/Bb1apVLa2K3nYLFSpk9erVs169etmvv/562tPv2bOndenSxXLkyGF169a16667zvLmzWsZfX/w008/WUbSv39/t1w33nijZUS7du2ynDlzumVctWpVnOM89dRT9umnn54wfMGCBfbYY4+5aSS3TZs2uddaunSppQXPP/+8W2ezZs2Kd5zXX3/djfP5559HDD/vvPPc8NGjRyfouzX69v3338d8eYIka2rPADKOIUOGWIUKFezgwYPug6kP73fffWcrVqxwO9aMQsFnkSJFUjU7F988XHLJJXbgwAHLnj17qsxX6dKlbejQoScMV9YzLbv88sutU6dO5nme7d6923755ReXtdV6fuaZZ6xv375Jnrbeo4suusiWLVvmfhg2bNgwpvOO5Kft4oMPPnDZxS+++ML+++8/O+OMMzLUqp80aZILqkqUKGHvvfeePfHEE3EGwPoB17Zt2xMC4MGDB7ttvUCBAskeAOu19F7UqVPnhEDz+PHjlpLat29v/fr1s/fff9+aNWsW5zh6rHDhwnbFFVeEhq1du9Z+/PFHtxxa3927dz/ld2u0ihUrxmgpgokAGDGjD7cO98rtt9/uAjQFD/rVe8MNNwRyTe/bt8/y5MmTYq+XOXPmVP2xoUD3lltuidl6UuChH1S5cuVK8jzp+fpBoHUTn8qVK58w308//bRdffXVdt9997kM9pVXXpnkedAXFV9WaZMCJpWnnOxzo5Kiv//+277++mtr0aKFTZ482Tp37myp6ejRo27eY/Vj991333XbeLly5VzAFlcAnNZly5YtxV+zVKlS1qRJE7dNKJOrIz3hdBT0m2++sTvvvDNi/rS+ixUrZs8995z7UfHnn3/GW74R/t2K2KEEAsnm4osvdv///vvvEcNXr17tPvA61KwvHX2wow8NiQ6n9enTx+0UtFNRdlFZuh07doTG2bZtm3Xt2tWKFy/uplW7dm2XuQunHYsyG88++6y99tprdvbZZ7vpNWjQwP0CD7dlyxa77bbb3GtpnJIlS1qbNm3cNETzsnLlSps3b17oMJQO/YcfrtJjd999t9u5aTonq03ToTw9J5p2jjo8ljt3bitYsKDL7M6YMeOU8xBfDbCyOzqsr0BSP0wU7EWXp2gedWhew5Xh0d9Fixa1+++/344dO2ax4i+zygtuuukmt3zKkPrLdtVVV9n06dPddqH5ffXVV91jf/zxh11//fVuu9F6Of/8823q1KkR0/aX/8MPP7SBAwe6jKvG3bNnT6LnUxkbTUdlEU8++WRouIKlRx991K1PBfwK3LWtz5kz54Rp7Ny50zp27Gj58uVzmTEFTcouax7DS1ji2w7iWrbw91bv+znnnOOyy40bN3bLqkD7o48+co9rG1HGWeuxSpUqJxym9bdZf/tOKQlZh/rxo+1Bn7+4ftToeXfddVdo2KFDh2zQoEFu+fXZLVOmjCtb0PBwWl6VpSjrVqNGDTfutGnTTjq/Grd69eou0FGWT/fD6QiBthfNs++ee+5xr/Xiiy+Ghm3dujXikHdCt6XwfdgLL7wQ2of5JToJ3afGZ8OGDfbtt9+6bKZu69evd1nd6PWmH6rav/r7He0ztO0qAyrKUvqPhW9T2p/5+x/No15j48aNEdP3t2Utk9aztmV9focNGxYaR9u+9tui/bT/Wv5nKa79rOZZP2K1PWid6XOg9Rj+XoVvFyrx0HxoXG0fp9o2RPtTHTmK3h+J9iH6oXLzzTdHDNePDL1n2t/pvdd9pCwywEg2/g5QAY5PgduFF17odmwPPvig2+FPnDjRBVwff/yxXXPNNW48nSSkLwLVoql+UrWTCny1U1cmRkGcDvVrp7lu3Tq349LOV4GedoIKnlXDGU47GB261JemdnbasV577bUusPJ/mbdr187No768tCNVgD1z5kz3BaH7+vLRYwoOH374YfccBd/hFPwqcNQXm3a+iaXDe/pSueCCC9yhL2V4fvjhB5d9at68eYLmIZy+HPRloS8OlSfoS3jkyJE2f/58d1JW+CFLBbrKcClo0peEAiZlKPSFe7JDdOHPD/+B4tMXX3SGV8FspUqV3GHV8C+jNWvWWIcOHdz7dMcdd7gvLM2z1sf+/fvt3nvvdcGGvohbt27tgj1/u/E9/vjjbr0peFcAlNQsWdmyZV1gqYBEQbQCWf3/xhtvuHnU/GmbGjt2rFtvixYtCh2W1ZeeMsgapnWnLPJnn30W88zhv//+675EFVRonSq40t8K0nr37m3dunVzPzSGDx/uvnAVeKT24fuErEN9RhVY6HP6zz//uMDJpzIETcPP2mtda1tQyZUybdWqVbPly5fbiBEj7LfffjuhblWfJe13tN/QvuRkJ05p+9G+SUGUaJ71edKPZZULiPZVei3tOxQ8iQJKHXXQ/9pm/WGiH7QJXQ/hxo0b54J/LaMCNK2ThO5TT0blHXqetiN9VvV51/ajz5zvnXfecUf29MNcry8aT8/TOtY0tA60PkX7QNGPx0ceecQdBdTzt2/fbi+99JJbB9H7H23LLVu2dPtlja/P9gMPPGA1a9Z0WVC9r9onat+qefCTLOHzGU77FW0X+vwqUaL1qR/XCtj1Q1/zG07bjzK52ofrM6IfL/pO0P5f+5z4aH71Gdd3jP4Op2HKqus98ml/ru8tvZ/aN+k5Wt8PPfRQnNNXcB29X9Xn42TzhATwgNM0btw4RS/erFmzvO3bt3sbN270PvroI69o0aJejhw53H1f06ZNvZo1a3oHDx4MDTt+/Lh3wQUXeJUqVQoNe/TRR900J0+efMLraXx54YUX3Djvvvtu6LHDhw97jRo18vLmzevt2bPHDVu/fr0br3Dhwt4///wTGvezzz5zw7/44gt3/99//3X3hw8fftLlrVGjhte4ceN418NFF13kHT16NOKxzp07e+XKlTvhOYMGDXLP8a1du9bLnDmzd80113jHjh2Lc7lPNg9z5sxx09P//vooVqyYd84553gHDhwIjTdlyhQ3ntZz+Dxq2JAhQyKmee6553r16tXzTkXzo+fHdbvrrrtOWOYOHTqcMA2tIz02bdq0iOG9e/d2w7/99tvQsP/++8+rUKGCV758+dC68pf/rLPO8vbv3+8lhMbv0aNHvI/36tXLjfPLL7+4+3pvDx06FDGOtp3ixYt7Xbp0CQ37+OOP3fO0nfo0n5dddpkbru0lep1Erwu9J/G9t+Hr/P333w8NW716tRum7ej7778PDZ8+ffoJr+tvs/qMxIo/zR9//DHecRK6DtesWeOmNXr06IhxW7du7d53/zPxzjvvuOUN3z5kzJgx7vnz588PDfPXzcqVKxO0PNqX6Tn6bIr2Kzlz5vRGjBgRGmfbtm1unFdeecXd37Vrl3uN66+/3i2T79577/UKFSoUmu+Ergd/H5YvXz73WuESuk89GT3/5ptvDt1/6KGHvCJFinhHjhyJGC9PnjwR26RP+8y4tqM///zTy5Ili/fkk09GDF++fLmXNWvWiOH+tvz222+HhmndlChRwmvXrl1omLar6O04vv3sp59+6sZ94oknIsa77rrrvEyZMnnr1q0LDdN42bNnjximz7yGv/TSS96p6L3WdrF79+4TPosDBgyIGLdnz55emTJlQtvBjBkz3HhLliyJ87MU103frTg9lEAgZnRoUL/6dahJmSZlBpSx9csAlMVR5kW/7JXp0C9a3XSYWBkPnRTgH5ZX5kLlDHFlL/xDxV9++aXLwCh74lMmV9kWZZB1+Deczt4Oz0b72QNlgEWZD/0a12E2ZSKSSpmcLFmyJOm5ylQpm6UMR3TN6qkOkcdFZ+Iri62MRniNY6tWrVxGMq5DdsoYhtN68tfRqSiTpox59E2ZyFO9jk+ZfG0P4fReK/Pkl0qIMuDKAulIQ3S3BmVZT6duOJzfsUHbrOi99TPKeq+0XaseU4edf/7559DzdOhU26O2B5/e0x49esRkvsLnTxlfnzLmyqopWxZ+wp3/d0Lfy+SU0HWo2mzNd3jJgcb96quv3CFl/zOhIz9aXm3T/n5Ft8suu8w9Hl1SoKy+ShoSQq+t+fJruJUZ1OcnfJ6039Nrq9ZTdHRFy6hMo45eaN/mZ4C1DfvzndD14FM20s+sJnafGh+VzyhbHr4f1d+ajrKlp0PZVC2X5i/8fdF+W0d/ot8XbcvhtfhaN/rcJ3Wb1X5D69jPwPuUzVfMq+0o+jtMWW1frVq13FGfhLy+5lvZeS2zzy9rCC9/0Ps7YcIE933kbwfaTlUyF11a4xs1atQJ+9ToeUfiUQKBmNGHVF9YOlzz5ptvui+D8BMCdMhHOx0dDtMtLgrWdChPdcPa2Z/MX3/95Xai0YGivgj9x6MPZ4fzg2E/2NW86qQ97RxVUqAaUx0SVN2xf6gzIeI6WzehtNxanoR+OZ+Kvw4UFEXTF7YO+YVTkBz+Beuvp4T+INCPnvjOhE7oeopruJYjru4J4e+1f+j5ZNNOCr9nb3jZgMovVBqi2ssjR47E+bqaJ9WQq5YxXKxPhtMPzOgfR6op1A/R6GGS2B93KjXSZzpcYj4P8UnIOhR9/lSqoPWpQ8kKdjW+aqt9CvRULhW97YbvV8JFv4YOy4fXuSsQ002lVAqi9Praf/l0OFs/0nXoX/s8/4eixvUDXQWxuqlMQfe1T1H9t8pRkrIe4hqWmH1qfFSfq8/tWWedFVpG7Qf87gQK9pNK74vmT/vphJy0Fte2rP2PgvSk0Dajk9SiS34S+h3hv35CPjMq0dB7raDX786jshAlclRL7NO5HNreFNiHb1Oqe9b4+g6K/k7TuJwEF3sEwIiZ8A+p6s+U6dDOXjWd+jLx29OoLjM6w+dLzjPl48vKhtefKlOpuk1lYpX90JeK6maVZTn33HMT9DpxZR7jy97G8uSyWEhq5jop4svQxiJzG6vsr6iNn9aLH3woYNAXnLZxZfiUudHj2k6iT/hMzfcsIdt7QihbpZrX05lGtMSsQ2W3dTKsXyOp52o/E/6jTvsW1YmqJ2tcon8MRG8fqo8PD4Z0Mp3q8BVsqwZYAapu0TRPqtkX7e/UhkvZQgW8Coj1uddw3Vcgpvn0jzwldj3ENd+nu0/127vpXIW4fnQreNYPwKT2rdb8aR0oWxnX9hg93Vhts0l1Oq+vYF6Zbm0Dyvqrblg/AMJP4hM/yxtfZyQduVQwjORHAIxk4e/E9UF++eWX3ckZyjD4O4pTZQl1GEqBx8koG6TMgHay4b+YlUnxH08KvbaywLppB6YTJ/Tlpy+rpJYiKIsQV5P46AyEXlvLo0P60SfAhEvoPPjrQD9C/MPBPg1L6jpKaZpPzW+0032vT0VfYvpCatSoUSiLpBNztC3rUGf4+6CgKXqedYhXJ+6FZ4HDsz7pgQIrHXKNpYSuQ1FWzS850KFklRfoRNDoz42yq02bNk3S51PTVqbb5++rNFxHFuKaL3UnUbbPD4D9wFbrSt1ltM8TneylExMVACvTqm4ISVkPcUnMPjUu2rZ1UrFOLPOzoj5lPVVipGSAX5YQ37qNb7jeFwWP+vHoZ8pPV2LeX30GdSJvdN/m5NpvaPscM2aM+9GoThqa1/DSEv3Q0ImwKn9QmWA0lWpomyMAThnUACPZqEODssL6slJtlLIbGqYvjs2bN58wvg4L+VT+oC+0Tz75JN5f4+pZqTOxtbMJr6/SGcbKLKjOLzEUqGg+o3fg2nGGt1LSl1hir3ik6egwcvihPK2D6OVTJkjBvL6Qohu6h2chEjoPypRpvWunHL4MysjokPHpHN5MSXqvdVb8woULI75M1NZOh2pjVTISTvWV+vJSlt7vthGeJQp/P3RWd/i8+YGjDmnrrH6fnhPfVZ/SKpVxKLgKv52uhK5Dn8od9KNQWVI9N7zm2c+mqdZV2bdoCmxP1Y1FJQ3hy6fAUt0yVMalaStYib4pK64fM5pvUZCnUgN1FtD77p/1r8BY2VwFuyqrUlu9pK6HaInZp56s/EHrNXr5VLuu0oXwutT49jt+h5fox9TdQMuoHwnRWVTdV61yYsX3WvHtN/T5VRImnN4jBafhF6aIBb3n2h9pvep7Sd9B/jkwov29tkWdBxDXNqWSO5XWRLfuQ/IgA4xkpR2rWjOpFZdOelKdsA4J6nCldrD6otHhIu3wlYlQ0Os/T18Yeq7aoClrooBEJ9UpmFNdlbIT2vHrEOLixYvdjkfP8TNEiW31pHo+ZZD0haeASl9U2mFp/sK/cDUvCmLUKF6HF/UlFJ1djabnq52PTurTr3wF25qGsiLhJ7toegq21MZLX5z6AlFtsjJKyiD5V1lL6DwoM6SaMn1Za2esgM5vg6b1pUPLsaQg38+UR0vKBTJ8yqbpUK2+sLT+lBVU7aSyLPrCONlFLhL63mu+9aWs1lTaDnX4W4d/dVhdrZl8+pJSxk7vpX5AaB60TWqb8euF/R8z+gGodazD4n4bNL8eNSmZyvRE5wHE1UNV7QkTug59Gkctn/SeaBvQ9h4dIKv1l/YxyrorEFHgo0yfhvt9pRND2V2/jVZ8wZX2EQoQ/fp0fWbV91X7N/8cA7Vw9FuFRdf/JnY9xCWh+9T42rvpKojxXQREy659hbZZrXPtd5RR1WdC+yMF/Vp2P6utfZf2ddrvqJRMP/y1jxowYIA7WVWfCe2XtZzat2ofrvKNxNA0dZKn1pOmpXWreYirZlrzoGyq5kuvr+8N1eDqc6hyt/AT3mJBn2m9x2rtKEpkhNO2ou04vrZtWt/6EaeTk8PbqSlh4Wetw2k6/lEAJMFpdpEATtr2SG2fzj77bHfzW4P9/vvvXqdOnVx7m2zZsnlnnnmmd9VVV7l2Q+F27tzp2sXocbWnKV26tGtzs2PHjtA4W7du9W677TbXskfjqJ1PdHscv4VQXO3NNFwtqETTVTusqlWrunY/+fPn9xo2bOhNnDgx4jlbtmzxWrVq5Z1xxhnu+X47slO1f1KrG7Uj03xWqVLFtW+Lq/2VvPnmm679mFrdFCxY0L3GzJkzTzkPcbXKkgkTJoSmpzZMann0999/R4yjdavljhbfPCamDVr48/3pqWVeNLUw0nLFRduN2hcVKFDAtRs677zzXDu3cP7yT5o0yUuo8HlU6ypNX+tK7c/iapWl1kVPPfWUm1etT42r+Yir1Z2W8aabbnLvk7anjh07et999517rQ8//PCEdZKUNmhqiZfQ9Rjd8i0526DFd1NbxMSsQ9/dd999Qsu3cGr598wzz7j14X9u1L5v8ODBEa2pTtX2zqd9SdmyZU86zqWXXuraDPrtwkaNGuWm371794jxmjVr5obPnj07YnhC18PJ9mGJ2aeG89v0jR07Nt5x5s6d68YZOXJkqK3XJZdc4uXKlcsND98+H3/8cfe6+gxFb1N6LbWH1P5FN+1j9R6ozd2ptuW4tgm1sKxevbprpRbeEi2ucdUusU+fPl6pUqXculFrOK3H8LaSJ9suoj+Hp6J9ht+mTC3twr+rNL/aB8RHrRtz587t2mAm5LMUVys4JFwm/ZOUwBkAkHjKPikTpg4c4c3xcXLKpKucRGVP0Z01ACCxqAEGgGQSfmKV6LC8ri6l3qI6NI6EUW2+SlR0bgDBL4BYoAYYAJKJLlmtIFhdJFRzqXrPBQsWuBrBWLZqy6hUe6qaU9X264Sp6MubA0BSEQADQDLRiYlqoTdlyhSXxdQJi+pSogsr4NTU+UGtpXQCljLnJ2sNCACJQQ0wAAAAAoUaYAAAAAQKATAAAAAChRrgBNAVuTZt2uSabmf05vUAAADpkTr76tLXulDLqS6QRACcAAp+y5QpE6v3BwAAAMlElzIPvwx1XAiAE8C/pK5WqPp3AgAAIG3RpeyVsPTjtpMhAE4Av+xBwS8BMAAAQNqVkHJVToIDAABAoBAAAwAAIFAIgAEAABAo1AADAIDAts06evSoHTt2LLVnBQmULVs2y5Ili50uAmAAABA4hw8fts2bN9v+/ftTe1aQyBPc1OIsb968djoIgAEAQOAucLV+/XqXSdRFE7Jnz86FrtJJxn779u32999/W6VKlU4rE0wADAAAApf9VRCsnrG5c+dO7dlBIhQtWtT+/PNPO3LkyGkFwJwEBwAAAulUl8tF+uzxmxC88wAAAAgUAmAAAAAECgEwAAAAks3cuXNd6cKuXbssreAkOAAAgP9n5NTlKbouerWqmajxb731Vnvrrbds6NCh9uCDD4aGf/rpp3bNNde4Tgk4NTLAAAAA6UjOnDntmWeesX///TemnTGChAAYAAAgHWnWrJmVKFHCZYHj8/HHH1uNGjUsR44cVr58eXvuueciHi9fvrw9/vjj1qlTJ8uXL5/deeedNn78eCtQoIBNmTLFqlSp4lrEXXfdde5iIco66zkFCxa0e++9N+Lqee+8847Vr1/fzjjjDDdfN910k23bts3SMgJgAACAdET9b5966il76aWX3EUhoi1evNhuuOEGa9++vS1fvtwee+wxe+SRR1yAG+7ZZ5+12rVr25IlS9zjomD3xRdftA8//NCmTZvm6ndVWvHll1+6m4LdV1991T766KPQdNSTV8H0L7/84kox1KdXpRppGTXA6bAOKbH1QgAAIGNRUFqnTh0bNGiQjR07NuKx559/3po2bRoKaitXrmy//vqrDR8+PCIwveyyy+y+++4L3f/2229dMDt69Gg7++yz3TBlgBX0bt261V1+uHr16takSRObM2eO3XjjjW6cLl26hKZx1llnuQC6QYMGtnfv3tO+ZHFyIQMMAACQDqkOWKUJq1atihiu+xdeeGHEMN1fu3ZtROlC/fr1T5imyh784FeKFy/uSh/CA1kNCy9xUMb56quvtrJly7oyiMaNG7vhGzZssLSKABgAACAduuSSS6xFixY2YMCAJD0/T548JwzLli1bxH21L4trmC4lLfv27XPzoDri9957z3788Uf75JNP0vyJdZRAAAAApFNPP/20K4XQSWu+atWq2fz58yPG032VQqh+OJZWr15tO3fudPNRpkwZN+ynn36ytI4MMAAAQDpVs2ZNu/nmm13drU91vbNnz3Ynpv3222+uTOLll1+2+++/P+avX7ZsWcuePbs7Ie+PP/6wzz//3L1uWkcGGAAAIB2faD5kyBCbMGFC6H7dunVt4sSJ9uijj7pgtGTJkm6c5OjMULRoUddd4qGHHnJBuF5b3SVat25taVkmj0uGnNKePXssf/78tnv3blfjklLoAgEAQOwdPHjQ1q9fbxUqVHAXlUDGeO8SE69RAgEAAIBAIQAGAABAoBAAAwAAIFAIgAEAABAoBMAAAAAIFAJgAAAABAoBMAAAAAKFABgAAACBQgAMAACAQOFSyAAAAL6Zd6Xsurj81UCt+/Lly1vv3r3dLTWRAQYAAEhnFi5caFmyZLFWrVql9qykSwTAAAAA6czYsWPtnnvusW+++cY2bdqU2rOT7hAAAwAApCN79+61CRMmWPfu3V0GePz48aHH5s6da5kyZbLZs2db/fr1LXfu3HbBBRfYmjVrIqYxevRoO/vssy179uxWpUoVe+eddyIe1zReffVVu+qqq9w0qlWr5rLO69ats0svvdTy5Mnjpvv777+HnqO/27RpY8WLF7e8efNagwYNbNasWfEuR5cuXdz0wx05csSKFSvmAvzkRAAMAACQjkycONGqVq3qAtdbbrnF3nzzTfM8L2Kchx9+2J577jn76aefLGvWrC7Y9H3yySfWq1cvu++++2zFihV211132W233WZz5syJmMbjjz9unTp1sqVLl7rXu+mmm9y4AwYMcNPVa/bs2TMiML/yyitd8L1kyRJr2bKlXX311bZhw4Y4l+P222+3adOm2ebNm0PDpkyZYvv377cbb7zRkhMBMAAAQDqi7KgCX1GQuXv3bps3b17EOE8++aQ1btzYqlevbg8++KAtWLDADh486B579tln7dZbb7W7777bKleubH379rVrr73WDQ+noPiGG25w4zzwwAP2559/2s0332wtWrRwGWEF0co4+2rXru0C5HPOOccqVarkAmhlmT///PM4l0MZ5Ojs87hx4+z66693GeTkRAAMAACQTqiUYdGiRdahQwd3X9ldZUujSwZq1aoV+rtkyZLu/23btrn/V61aZRdeeGHE+Lqv4fFNQ2UNUrNmzYhhCqr37NkTygDff//9LjguUKCAC2I1zfgywH4WWEGvbN261b766quIbHVyoQ0aAABAOqFA9+jRo1aqVKnQMJUi5MiRw15++eXQsGzZskXU88rx48cT9VrZ4pjGyaar4HfmzJkuk1yxYkXLlSuXXXfddXb48OF4X0MlFspQq75YWeoKFSrYxRdfbMmNABgAACAdUOD79ttvu9re5s2bRzzWtm1b++CDD1yt7qlUq1bN5s+fb507dw4N032VS5wOTUOlFddcc00oI6yyiZMpXLiwm3dlgRUEq+wiJRAAAwAApAM6Qezff/+1rl27Wv78+SMea9euncsODx8+/JTT6devn6vtPffcc61Zs2b2xRdf2OTJk0/asSEhVPer6ejEN2WHH3nkkQRlnVUGoW4Qx44diwjKkxMBMAAAQDq4MpsCXAWs0cGvHwAPGzbMli1bdsrptG3b1kaOHOlKFXQim8oOlIFVe7PT8fzzz7v6XZ3cVqRIEXfinF8ffDJaJtUp16hRI6K0Izll8qL7ZuAEevO0seksy3z58qXYGho5dXmcw3u1+v8F6AAAIHF04tb69etd4JczZ05WXypTqcSZZ57pgnB1o0jqe5eYeI0MMAAAAFKcyiN27NjhaprVNaJ169Yp9toEwAAAAEhxao+mTG7p0qXd1ezU0i2lEAADAAAgxZUvX/6EK9ilFC6EAQAAgEAhAAYAAIFEH4DgvmepGgA/9thjrk9c+C28gbPO9OvRo4drkqzL6anFhy6TF10/0qpVK8udO7cVK1bM9bZTo+hwuk513bp13VVSdGUS1ZkAAIBg8q9mtn///tSeFSSSf1W5LFmyWLquAVbPt/DGy+EF0H369LGpU6fapEmTXFuLnj17uvYYutKIqGGygt8SJUq4y+dt3rzZXVJPG/ZTTz3lxlGrDI3TrVs3e++992z27Nmu4bL6zbVo0SIVlhgAAKQmBU/qOrBt2zZ3X0k0/7K+SNtdI7Zv3+7er9M9YS7VA2AtgALYaOrhpobP77//vl122WVumPrD6fJ933//vZ1//vk2Y8YM+/XXX10AXbx4catTp449/vjjrvGyssvZs2e3MWPGuDMM1WJD9PzvvvvORowYQQAMAEBA+bGHHwQjfcicObOVLVv2tH+wpHoAvHbtWnfVDzUzbtSokQ0dOtQt2OLFi+3IkSPu6iA+lUfoMV0rWgGw/q9Zs6YLfn3K6nbv3t1WrlzpLvGnccKn4Y/Tu3fveOfp0KFD7uZLyFVMAABA+qEASkeDVT6peAPpg5KbCoJPV6oGwA0bNnT1uFWqVHHlC4MHD7aLL77YVqxYYVu2bHELqUMU4RTs6jHR/+HBr/+4/9jJxlFQe+DAAcuVK9cJ86UgXPMCAAAyfjnE6daTIv1J1QD4iiuuCP1dq1YtFxCXK1fOJk6cGGdgmlIGDBhgffv2Dd1XsFymTJlUmx8AAABk0DZoyvZWrlzZ1q1b52pzdKbfrl27IsZRFwi/bkf/R3eF8O+fahxdIzq+IFvdIvR4+A0AAAAZQ5oKgPfu3Wu///67q8mpV6+e6+agrg2+NWvWuLZnqhUW/b98+fKIAvaZM2e6gLV69eqhccKn4Y/jTwMAAADBkqoB8P3332/z5s2zP//807Uxu+aaa1wdTocOHVzbs65du7pShDlz5riT4m677TYXuOoEOGnevLkLdDt27Gi//PKLTZ8+3QYOHOh6ByuLK2p/9scff1j//v1t9erV9sorr7gSC7VYAwAAQPCkag3w33//7YLdnTt3WtGiRe2iiy5yLc70t6hVmc700wUw1JVB3RsUwPoULE+ZMsV1fVBgnCdPHuvcubMNGTIkNI5aoKmXsALekSNHWunSpe2NN96gBRoAAEBAZfK4DuAp6SQ4ZaTVmzgl64FHTl0e5/BerWqm2DwAAABktHgtTdUAAwAAAMmNABgAAACBQgAMAACAQCEABgAAQKAQAAMAACBQCIABAAAQKATAAAAACBQCYAAAAAQKATAAAAAChQAYAAAAgUIADAAAgEAhAAYAAECgEAADAAAgUAiAAQAAECgEwAAAAAgUAmAAAAAECgEwAAAAAoUAGAAAAIFCAAwAAIBAIQAGAABAoBAAAwAAIFAIgAEAABAoBMAAAAAIFAJgAAAABAoBMAAAAAKFABgAAACBQgAMAACAQCEABgAAQKAQAAMAACBQCIABAAAQKATAAAAACBQCYAAAAAQKATAAAAAChQAYAAAAgUIADAAAgEAhAAYAAECgEAADAAAgUAiAAQAAECgEwAAAAAgUAmAAAAAECgEwAAAAAoUAGAAAAIFCAAwAAIBAIQAGAABAoBAAAwAAIFAIgAEAABAoBMAAAAAIFAJgAAAABAoBMAAAAAKFABgAAACBQgAMAACAQCEABgAAQKAQAAMAACBQCIABAAAQKATAAAAACJQ0EwA//fTTlilTJuvdu3do2MGDB61Hjx5WuHBhy5s3r7Vr1862bt0a8bwNGzZYq1atLHfu3FasWDHr16+fHT16NGKcuXPnWt26dS1HjhxWsWJFGz9+fIotFwAAANKWNBEA//jjj/bqq69arVq1Iob36dPHvvjiC5s0aZLNmzfPNm3aZNdee23o8WPHjrng9/Dhw7ZgwQJ76623XHD76KOPhsZZv369G6dJkya2dOlSF2DffvvtNn369BRdRgAAAKQNqR4A7927126++WZ7/fXXrWDBgqHhu3fvtrFjx9rzzz9vl112mdWrV8/GjRvnAt3vv//ejTNjxgz79ddf7d1337U6derYFVdcYY8//riNGjXKBcUyZswYq1Chgj333HNWrVo169mzp1133XU2YsSIVFtmAAAABDgAVomDMrTNmjWLGL548WI7cuRIxPCqVata2bJlbeHChe6+/q9Zs6YVL148NE6LFi1sz549tnLlytA40dPWOP404nLo0CE3jfAbAAAAMoasqfniH374of3888+uBCLali1bLHv27FagQIGI4Qp29Zg/Tnjw6z/uP3aycRTUHjhwwHLlynXCaw8dOtQGDx4cgyUEAABAWpNqGeCNGzdar1697L333rOcOXNaWjJgwABXguHfNK8AAADIGFItAFaJw7Zt21x3hqxZs7qbTnR78cUX3d/K0qqOd9euXRHPUxeIEiVKuL/1f3RXCP/+qcbJly9fnNlfUbcIPR5+AwAAQMaQagFw06ZNbfny5a4zg3+rX7++OyHO/ztbtmw2e/bs0HPWrFnj2p41atTI3df/moYCad/MmTNdwFq9evXQOOHT8MfxpwEAAIBgSbUa4DPOOMPOOeeciGF58uRxPX/94V27drW+fftaoUKFXFB7zz33uMD1/PPPd483b97cBbodO3a0YcOGuXrfgQMHuhPrlMWVbt262csvv2z9+/e3Ll262Ndff20TJ060qVOnpsJSAwAAINAnwZ2KWpVlzpzZXQBDnRnUveGVV14JPZ4lSxabMmWKde/e3QXGCqA7d+5sQ4YMCY2jFmgKdtVTeOTIkVa6dGl744033LQAAAAQPJk8z/NSeybSOnWMyJ8/vzshLiXrgUdOXR7n8F6taqbYPAAAAGS0eC3V+wADAAAAKYkAGAAAAIFCAAwAAIBAIQAGAABAoBAAAwAAIFAIgAEAABAoBMAAAAAIFAJgAAAABAoBMAAAAAIlTV8KGXHjCnEAAABJRwYYAAAAgUIADAAAgEAhAAYAAECgEAADAAAgUAiAAQAAECgEwAAAAAgUAmAAAAAECgEwAAAAAoUAGAAAAIFCAAwAAIBAIQAGAABAoBAAAwAAIFAIgAEAABAoBMAAAAAIFAJgAAAABAoBMAAAAAKFABgAAACBQgAMAACAQCEABgAAQKAQAAMAACBQCIABAAAQKATAAAAACBQCYAAAAAQKATAAAAAChQAYAAAAgUIADAAAgEAhAAYAAECgEAADAAAgUAiAAQAAECgEwAAAAAgUAmAAAAAECgEwAAAAAiVJAfDPP/9sy5cvD93/7LPPrG3btvbQQw/Z4cOHYzl/AAAAQOoHwHfddZf99ttv7u8//vjD2rdvb7lz57ZJkyZZ//79YzuHAAAAQGoHwAp+69Sp4/5W0HvJJZfY+++/b+PHj7ePP/44lvMHAAAApH4A7HmeHT9+3P09a9Ysu/LKK93fZcqUsR07dsR2DgEAAIDUDoDr169vTzzxhL3zzjs2b948a9WqlRu+fv16K168eCznDwAAAEj9APiFF15wJ8L17NnTHn74YatYsaIb/tFHH9kFF1wQ2zkEAAAAYihrUp5Uq1atiC4QvuHDh1uWLFliMV8AAABA2gmAfWp5tm3btlA9sK9s2bKnO18AAABA2gmA1QWia9eutmDBghNOjsuUKZMdO3YsVvMHAAAApH4AfNttt1nWrFltypQpVrJkSRf0AgAAABk2AF66dKktXrzYqlatGvs5AgAAANJaF4jq1avT7xcAAADBCYCfeeYZd8njuXPn2s6dO23Pnj0RNwAAACBDBcDNmjWz77//3po2bWrFihWzggULuluBAgXc/wk1evRo11ItX7587taoUSP76quvQo8fPHjQevToYYULF7a8efNau3btbOvWrRHT2LBhg7sQR+7cud289OvXz44ePRoxjgL1unXrWo4cOVzPYl2yGQAAAMGUpBrgOXPmxOTFS5cubU8//bRVqlTJdZB46623rE2bNrZkyRKrUaOG9enTx6ZOnWqTJk2y/PnzuwtvXHvttTZ//nz3fHWbUPBbokQJ15Fi8+bN1qlTJ8uWLZs99dRToavTaZxu3brZe++9Z7Nnz7bbb7/dnbzXokWLmCwHAAAA0o9MniLPNKRQoULughrXXXedFS1a1N5//333t6xevdqqVatmCxcutPPPP99li6+66irbtGlT6BLMY8aMsQceeMC2b99u2bNnd38riF6xYkXoNdq3b2+7du2yadOmJWieVNahAHz37t0uU51SRk498WIjJ9OrVc1kmxcAAIC0LDHxWpJKIHz79+93QemyZcsibkmhbO6HH35o+/btc6UQ6jJx5MgRV27hU9cJXWRDAbDo/5o1a4aCX1FWVytg5cqVoXHCp+GP408jLocOHaKuGQAAIINKUgmEsqvqBRxerxsuMRfC0CWVFfCq3ld1vp988onrMqFWa8rgqq44nILdLVu2uL/1f3jw6z/uP3aycRQkHzhwwHLlynXCPA0dOtQGDx6c4GUAAABA+pGkDHDv3r1dCcEPP/zgAkiVEqh+V7W8n3/+eaKmVaVKFRfsalrdu3e3zp0726+//mqpacCAAS597t82btyYqvMDAACAVM4Af/311/bZZ59Z/fr1LXPmzFauXDm7/PLLXb2Fsqc66SyhlOVVZwapV6+e/fjjjzZy5Ei78cYb7fDhwy7QDs8CqwuETnoT/b9o0aKI6fldIsLHie4cofua17iyv6JuEboBAAAg40lSBlh1umo5Jmp7ppIIUT3uzz//fFozdPz4cVeDq2BY3RzUtcG3Zs0a1/ZMJROi/1VCsW3bttA4M2fOdMGtyij8ccKn4Y/jTwMAAADBkqQMsMoWFIyWL1/eateuba+++qr7Wx0Y1F4sMaUGV1xxhTux7b///nMdH9Szd/r06e4svq5du1rfvn1dZwgFtffcc48LXNUBQpo3b+4C3Y4dO9qwYcNcve/AgQNd72A/g6v2Zy+//LK7cEeXLl1c9nrixImuMwQAAACCJ0kBcK9evVzPXRk0aJC1bNnS9dhVOUNiLjKhzK369mpaCnh1UQwFvyqnkBEjRrgSC10AQ1lhdW945ZVXQs/PkiWLTZkyxdUOKzDOkyePqyEeMmRIaJwKFSq4YFc9hVVaod7Db7zxBj2AAQAAAiomfYD9dmjK5BYpUsQymvTSBzg+9AcGAAAZ3Z7k7gP8wQcfRNzXZYh1qWEFv7oUMQAAAJBWJSkAVslBXD2AVWbw7rvvxmK+AAAAgLQTAKvet0OHDvbdd9+FhukENZ1cNmfOnFjOHwAAAJD6AbD6/OpktNatW7tLFt999902efJkF/zqcsUAAABAhuoCITfddJO7SMWFF15oRYsWtXnz5oUuaAEAAACk+wBY/XjjouBXJ8CFtyd7/vnnYzN3AAAAQGoFwEuWLIlzuLK+ajvhP54pU6bYzR0AAACQWgEwJ7cBAAAgsCfB+datW+eu3HbgwAF3PwbX1AAAAADSXgC8c+dOa9q0qVWuXNmuvPLK0GWRu3btavfdd1+s5xEAAABI3QBYF7zIli2bbdiwwV0FznfjjTfatGnTYjd3AAAAQFpogzZjxgxX+lC6dOmI4ZUqVbK//vorVvMGAAAApI0M8L59+yIyv75//vnHcuTIEYv5AgAAANJOAHzxxRfb22+/Hbqv1mfHjx+3YcOGWZMmTWI5fwAAAEDql0Ao0NVJcD/99JMdPnzY+vfvbytXrnQZ4Pnz58d2DgEAAIDUzgCfc8459ttvv9lFF11kbdq0cSUR1157rbsYxtlnnx3L+QMAAABSNwN85MgRa9mypY0ZM8Yefvjh2M4NAAAAkNYywGp/tmzZsuSZGwAAACAtlkDccsstNnbs2NjPDQAAAJAWT4I7evSovfnmmzZr1iyrV6+e5cmTJ+Lx559/PlbzBwAAAKReAPzHH39Y+fLlbcWKFVa3bl03TCfDhVNLNAAAACBDBMC60tvmzZttzpw5oUsfv/jii1a8ePHkmj8AAAAg9WqAPc+LuP/VV1+5FmgAAABAhj4JLr6AGAAAAMhQAbDqe6NrfKn5BQAAQIatAVbG99Zbb7UcOXK4+wcPHrRu3bqd0AVi8uTJsZ1LAAAAIDUC4M6dO5/QDxgAAADIsAHwuHHjkm9OAAAAgLR+EhwAAACQ3hAAAwAAIFAIgAEAABAoBMAAAAAIFAJgAAAABAoBMAAAAAKFABgAAACBQgAMAACAQCEABgAAQKAQAAMAACBQCIABAAAQKATAAAAACBQCYAAAAAQKATAAAAAChQAYAAAAgUIADAAAgEAhAAYAAECgEAADAAAgUAiAAQAAECgEwAAAAAgUAmAAAAAECgEwAAAAAoUAGAAAAIFCAAwAAIBAIQAGAABAoBAAAwAAIFAIgAEAABAoqRoADx061Bo0aGBnnHGGFStWzNq2bWtr1qyJGOfgwYPWo0cPK1y4sOXNm9fatWtnW7dujRhnw4YN1qpVK8udO7ebTr9+/ezo0aMR48ydO9fq1q1rOXLksIoVK9r48eNTZBkBAACQtqRqADxv3jwX3H7//fc2c+ZMO3LkiDVv3tz27dsXGqdPnz72xRdf2KRJk9z4mzZtsmuvvTb0+LFjx1zwe/jwYVuwYIG99dZbLrh99NFHQ+OsX7/ejdOkSRNbunSp9e7d226//XabPn16ii8zAAAAUlcmz/M8SyO2b9/uMrgKdC+55BLbvXu3FS1a1N5//3277rrr3DirV6+2atWq2cKFC+3888+3r776yq666ioXGBcvXtyNM2bMGHvggQfc9LJnz+7+njp1qq1YsSL0Wu3bt7ddu3bZtGnTTjlfe/bssfz587v5yZcvn6WUkVOXx2Q6vVrVjMl0AAAA0qrExGtpqgZYMyyFChVy/y9evNhlhZs1axYap2rVqla2bFkXAIv+r1mzZij4lRYtWriVsHLlytA44dPwx/GnEe3QoUPu+eE3AAAAZAxpJgA+fvy4K0248MIL7ZxzznHDtmzZ4jK4BQoUiBhXwa4e88cJD379x/3HTjaOAtsDBw7EWZusXxD+rUyZMjFeWgAAAFjQA2DVAqtE4cMPP0ztWbEBAwa4bLR/27hxY2rPEgAAAGIkq6UBPXv2tClTptg333xjpUuXDg0vUaKEO7lNtbrhWWB1gdBj/jiLFi2KmJ7fJSJ8nOjOEbqv+pBcuXKdMD/qFKEbAAAAMp5UzQDr/DsFv5988ol9/fXXVqFChYjH69WrZ9myZbPZs2eHhqlNmtqeNWrUyN3X/8uXL7dt27aFxlFHCQW31atXD40TPg1/HH8aAAAACI6sqV32oA4Pn332mesF7Nfsqu5WmVn937VrV+vbt687MU5B7T333OMCV3WAELVNU6DbsWNHGzZsmJvGwIED3bT9LG63bt3s5Zdftv79+1uXLl1csD1x4kTXGQIAAADBkqoZ4NGjR7sa20svvdRKliwZuk2YMCE0zogRI1ybM10AQ63RVM4wefLk0ONZsmRx5RP6X4HxLbfcYp06dbIhQ4aExlFmWcGusr61a9e25557zt544w3XCQIAAADBkqb6AKdV9AEGAABI29JtH2AAAAAguREAAwAAIFAIgAEAABAoBMAAAAAIFAJgAAAABAoBMAAAAAKFABgAAACBkqpXgkPKGDl1eZzDe7WqyVsAAAAChwwwAAAAAoUAGAAAAIFCAAwAAIBAIQAGAABAoBAAAwAAIFAIgAEAABAoBMAAAAAIFAJgAAAABAoBMAAAAAKFABgAAACBQgAMAACAQCEABgAAQKAQAAMAACBQCIABAAAQKATAAAAACBQCYAAAAAQKATAAAAAChQAYAAAAgUIADAAAgEAhAAYAAECgEAADAAAgUAiAAQAAECgEwAAAAAgUAmAAAAAECgEwAAAAAoUAGAAAAIFCAAwAAIBAIQAGAABAoBAAAwAAIFAIgAEAABAoBMAAAAAIFAJgAAAABAoBMAAAAAKFABgAAACBQgAMAACAQCEABgAAQKAQAAMAACBQCIABAAAQKATAAAAACBQCYAAAAARK1tSeAQCnaeZdpx7n8ldZzQAA/D9kgAEAABAoBMAAAAAIFAJgAAAABAo1wAE2curyOIf3alUzxecFAAAgpZABBgAAQKAQAAMAACBQUjUA/uabb+zqq6+2UqVKWaZMmezTTz+NeNzzPHv00UetZMmSlitXLmvWrJmtXbs2Ypx//vnHbr75ZsuXL58VKFDAunbtanv37o0YZ9myZXbxxRdbzpw5rUyZMjZs2LAUWT6kwXZhp7oBAIAML1UD4H379lnt2rVt1KhRcT6uQPXFF1+0MWPG2A8//GB58uSxFi1a2MGDB0PjKPhduXKlzZw506ZMmeKC6jvvvDP0+J49e6x58+ZWrlw5W7x4sQ0fPtwee+wxe+2111JkGQEAAJC2pOpJcFdccYW7xUXZ3xdeeMEGDhxobdq0ccPefvttK168uMsUt2/f3latWmXTpk2zH3/80erXr+/Geemll+zKK6+0Z5991mWW33vvPTt8+LC9+eablj17dqtRo4YtXbrUnn/++YhAGUiTyEoDABCcGuD169fbli1bXNmDL3/+/NawYUNbuHChu6//VfbgB7+i8TNnzuwyxv44l1xyiQt+fcoir1mzxv799984X/vQoUMucxx+AwAAQMaQZgNgBb+ijG843fcf0//FihWLeDxr1qxWqFChiHHimkb4a0QbOnSoC7b9m+qGAQAAkDGk2QA4NQ0YMMB2794dum3cuDG1ZwkAAAAZPQAuUaKE+3/r1q0Rw3Xff0z/b9u2LeLxo0ePus4Q4ePENY3w14iWI0cO11Ui/AYAAICMIc1eCa5ChQouQJ09e7bVqVPHDVMtrmp7u3fv7u43atTIdu3a5bo71KtXzw37+uuv7fjx465W2B/n4YcftiNHjli2bNncMHWMqFKlihUsWDDVli8ouNocAABIa1I1A6x+verIoJt/4pv+3rBhg+sL3Lt3b3viiSfs888/t+XLl1unTp1cZ4e2bdu68atVq2YtW7a0O+64wxYtWmTz58+3nj17ug4RGk9uuukmdwKc+gOrXdqECRNs5MiR1rdv39RcdAAAAAQxA/zTTz9ZkyZNQvf9oLRz5842fvx469+/v+sVrHZlyvRedNFFru2ZLmjhU5szBb1NmzZ13R/atWvnegf7dBLbjBkzrEePHi5LXKRIEXdxDVqgAQAABFOqBsCXXnqp6/cbH2WBhwwZ4m7xUceH999//6SvU6tWLfv2229Pa14BAACQMaTZk+AAAACA5EAADAAAgEBJs10ggEThksEAACCByAADAAAgUAiAAQAAECgEwAAAAAgUaoCRrFd8AwAASGvIAAMAACBQCIABAAAQKATAAAAACBQCYAAAAAQKATAAAAAChS4QSFNdI3q1qpni8wIAAIKFDDAAAAAChQAYAAAAgUIADAAAgEAhAAYAAECgcBIcEG7mXadeH5e/yjoDACAdIwMMAACAQCEABgAAQKBQAoEE9+gV+vQCAID0jgwwAAAAAoUAGAAAAIFCAAwAAIBAIQAGAABAoBAAAwAAIFAIgAEAABAotEED0vJV51LytbjCHQAgIAiAkaou2zIkcsDMQieORGAGAABiiBIIAAAABAoBMAAAAAKFEgikXHlDRkE9LQAA6RoZYAAAAAQKATAAAAAChQAYAAAAgUINMBJl5NTlrDEAAJCukQEGAABAoJABBtL7Vd4AAECikAEGAABAoJABRpJk2B6/AAAgwyMARpqyfMM/Jwz7eupy69WqZqrMDwAAyHgIgAH8X1zhDgAQENQAAwAAIFDIAAPI+FlpstsAgDAEwECMa5alZtlCrFcAANIoAmAA6bsHcqxejywxAAQGATDSR8u1mWRU0wQu8AEAyAA4CQ4AAACBQgAMAACAQCEABgAAQKBQAwwksdMDAABInwiAke6D0MS2HCOgBQAg2AiAkWGlZqBLf2AAANIuaoABAAAQKGSAke5R0gAAABIjUAHwqFGjbPjw4bZlyxarXbu2vfTSS3beeeel9mylzQtPIFlQGgEAQOoLTAA8YcIE69u3r40ZM8YaNmxoL7zwgrVo0cLWrFljxYoVS+3ZQ8AlNjCO5UmBAAAETSbP8zwLAAW9DRo0sJdfftndP378uJUpU8buuecee/DBB0/63D179lj+/Plt9+7dli9fvhSaY7ORU5fHfJpkd3G6Ah1gX/5qas8BACAG8VogMsCHDx+2xYsX24ABA0LDMmfObM2aNbOFCxeeMP6hQ4fczacV6a/YlHRw/96YT3PvgSMxnyaCZeGarRZYa9qmysvWKF0wUeOv/Pvf0N/fFH8g9Hf3FjViOl8AkJb4cVpCcruBCIB37Nhhx44ds+LFi0cM1/3Vq1efMP7QoUNt8ODBJwxXxhgA0pfPQn+d/FgXAGQM//33n8sEW9AD4MRSplj1wj6VS/zzzz9WuHBhy5QpU4r9ilHAvXHjxhQtu8joWK+s2/SGbZZ1m96wzbJuU4syvwp+S5UqdcpxAxEAFylSxLJkyWJbt0YeutX9EiVKnDB+jhw53C1cgQIFLDUo+CUAZr2mJ2yzrNf0hm2W9ZresM3G71SZ30BdCCN79uxWr149mz17dkRWV/cbNWqUqvMGAACAlBWIDLCopKFz585Wv3591/tXbdD27dtnt912W2rPGgAAAFJQYALgG2+80bZv326PPvqouxBGnTp1bNq0aSecGJdWqARj0KBBJ5RigPWaVrHNsl7TG7ZZ1mt6wzYbO4HpAwwAAAAEpgYYAAAA8BEAAwAAIFAIgAEAABAoBMAAAAAIFALgFDJq1CgrX7685cyZ0xo2bGiLFi066fiTJk2yqlWruvFr1qxpX375ZcTjOndRHS1KlixpuXLlsmbNmtnatWstiGK9bidPnmzNmzcPXflv6dKlFkSxXK9HjhyxBx54wA3PkyePu0pPp06dbNOmTRZEsd5mH3vsMfe41m3BggXd/uCHH36woIn1eg3XrVs3tz9QC80givW6vfXWW936DL+1bNnSgiY5ttlVq1ZZ69at3QUhtE9o0KCBbdiwIRmXIp1SFwgkrw8//NDLnj279+abb3orV6707rjjDq9AgQLe1q1b4xx//vz5XpYsWbxhw4Z5v/76qzdw4EAvW7Zs3vLly0PjPP30017+/Pm9Tz/91Pvll1+81q1bexUqVPAOHDgQqLczOdbt22+/7Q0ePNh7/fXX1SHFW7JkiRc0sV6vu3bt8po1a+ZNmDDBW716tbdw4ULvvPPO8+rVq+cFTXJss++99543c+ZM7/fff/dWrFjhde3a1cuXL5+3bds2LyiSY736Jk+e7NWuXdsrVaqUN2LECC9okmPddu7c2WvZsqW3efPm0O2ff/7xgiQ51uu6deu8QoUKef369fN+/vlnd/+zzz6Ld5pBRgCcAvRF36NHj9D9Y8eOuR3p0KFD4xz/hhtu8Fq1ahUxrGHDht5dd93l/j5+/LhXokQJb/jw4aHHFWDkyJHD++CDD7wgifW6Dbd+/frABsDJuV59ixYtcuv3r7/+8oIkJdbt7t273bqdNWuWFxTJtV7//vtv78wzz3Q/LMqVKxfIADg51q0C4DZt2nhBlhzr9cYbb/RuueWWZJzrjIMSiGR2+PBhW7x4sTsk6cucObO7v3Dhwjifo+Hh40uLFi1C469fv95dzCN8HB3q0OGT+KaZESXHukXKrdfdu3e7w54FChQIzGpPiXWr13jttdfcPqF27doWBMm1Xo8fP24dO3a0fv36WY0aNSyIknObnTt3rhUrVsyqVKli3bt3t507d1pQJMd61fY6depUq1y5shuudau44NNPP03mpUmfCICT2Y4dO+zYsWMnXHFO9xXExkXDTza+/39ippkRJce6Rcqs14MHD7qa4A4dOli+fPkCs9qTc91OmTLF8ubN62oDR4wYYTNnzrQiRYpYECTXen3mmWcsa9asdu+991pQJde6Vb3v22+/bbNnz3bred68eXbFFVe41wqC5Fiv27Zts71799rTTz/t1u+MGTPsmmuusWuvvdatXwT0UsgA0gadEHfDDTe4EzlHjx6d2rOTYTRp0sSdsKkv1tdff92tY50IpywQEk/ZuZEjR9rPP//sjlQgttq3bx/6Wydz1apVy84++2yXFW7atCmrOwmUAZY2bdpYnz593N916tSxBQsW2JgxY6xx48as1zBkgJOZMjBZsmSxrVu3RgzX/RIlSsT5HA0/2fj+/4mZZkaUHOsWybte/eD3r7/+chnKIGV/k3vd6mzvihUr2vnnn29jx451mUv9HwTJsV6//fZbl1ErW7asW5e6abu977773Fn7QZFS+9mzzjrLvda6dessCJJjvWqa2k6rV68eMU61atXoAhEHAuBklj17dqtXr547zBP+K033GzVqFOdzNDx8fFGw4I9foUIFt8GHj7Nnzx6X7YlvmhlRcqxbJN969YNfteubNWuWazMXNCm5zWq6hw4dsiBIjvWq2t9ly5a5rLp/U/s+1QNPnz7dgiKlttm///7b1QCrtWcQJMd61TTV8mzNmjUR4/z2229Wrly5ZFmOdC21z8ILSqsTdWgYP368a11y5513ulYnW7ZscY937NjRe/DBByNanWTNmtV79tlnvVWrVnmDBg2Ksw2apqH2JsuWLXNn0wa1DVqs1+3OnTtd54epU6e6M+n1GrqvNj1BEev1evjwYdeqr3Tp0t7SpUsjWh8dOnTIC5JYr9u9e/d6AwYMcK3l/vzzT++nn37ybrvtNvca6lwQFMmxL4gW1C4QsV63//33n3f//fe7bVbddtStpG7dul6lSpW8gwcPekGRHNusWvZp2GuvveatXbvWe+mll1zrtG+//TZVljEtIwBOIdoIy5Yt63r+qfXJ999/H3qscePGriVMuIkTJ3qVK1d249eoUcMFY+HUCu2RRx7xihcv7j5ATZs29dasWeMFUazX7bhx41zgG33TziZIYrle/ZZycd3mzJnjBU0s161+9F5zzTWufZIeL1mypPuxoTZzQRPrfUG0oAbAsV63+/fv95o3b+4VLVrUBWtar+qB6wd+QZIc2+zYsWO9ihUrejlz5nT9q3W9AJwok/5J7Sw0AAAAkFKoAQYAAECgEAADAAAgUAiAAQAAECgEwAAAAAgUAmAAAAAECgEwAAAAAoUAGAAAAIFCAAwAAIBAIQAGACTIn3/+aZkyZbKlS5eyxgCkawTAADK8W2+91QVuumXLls0qVKhg/fv3t4MHD1p6MXfuXDf/u3btSrF11rZt24hhZcqUsc2bN9s555yTrK/92GOPhd6v8FvVqlWT9XUBBEfW1J4BAEgJLVu2tHHjxtmRI0ds8eLF1rlzZxdUPfPMMxnqDTh8+LBlz549WaadJUsWK1GihKWEGjVq2KxZsyKGZc2aNVHLfezYMfceZ86cuFxPUp8HIP3g0w0gEHLkyOGCN2Uxldls1qyZzZw5M/T48ePHbejQoS47nCtXLqtdu7Z99NFHEdNYuXKlXXXVVZYvXz4744wz7OKLL7bff/899PwhQ4ZY6dKl3WvVqVPHpk2bdkL5wOTJk61JkyaWO3du9xoLFy4MjfPXX3/Z1VdfbQULFrQ8efK4IPDLL790z9VzRI9pOsrQyqWXXmo9e/a03r17W5EiRaxFixZxliooc6xhyiSfanmUgX3rrbfss88+C2Vf9by4pjtv3jw777zz3DKXLFnSHnzwQTt69Gjocc3fvffe6zLuhQoVcu+Bpn8qCnY1bvhNy+crX768Pf7449apUyc3/3feeaeNHz/eChQoYJ9//rlVr17dzdOGDRvs33//deNp3Wm9X3HFFbZ27drQtOJ7HoCMiwAYQOCsWLHCFixYEJExVPD79ttv25gxY1xg2KdPH7vllltcgCf/+9//7JJLLnHB0ddff+2yyF26dAkFeyNHjrTnnnvOnn32WVu2bJkLRFu3bh0RaMnDDz9s999/vwsiK1eubB06dAhNo0ePHnbo0CH75ptvbPny5S47nTdvXhe0f/zxx26cNWvWuDIEvZ5PwaqWZf78+W7+E+Jky6P5u+GGG1zWXK+l2wUXXBDnNK688kpr0KCB/fLLLzZ69GgbO3asPfHEExHjaf4U0P/www82bNgw90Mh/MdHUmld60fEkiVL7JFHHnHD9u/f79bbG2+84d7HYsWKuR8LP/30kwtw9YPD8zw33zoa4IvreQAyMA8AMrjOnTt7WbJk8fLkyePlyJHD064vc+bM3kcffeQeP3jwoJc7d25vwYIFEc/r2rWr16FDB/f3gAEDvAoVKniHDx+O8zVKlSrlPfnkkxHDGjRo4N19993u7/Xr17vXfeONN0KPr1y50g1btWqVu1+zZk3vsccei3P6c+bMceP++++/EcMbN27snXvuuRHD/NdasmRJaJiep2GaTkKWR+usTZs2J53uQw895FWpUsU7fvx4aJxRo0Z5efPm9Y4dOxaav4suuuiE9fLAAw948Rk0aJB7f/R+hd/uuuuu0DjlypXz2rZtG/G8cePGuflbunRpaNhvv/3mhs2fPz80bMeOHV6uXLm8iRMnxvs8ABkbNcAAAkElBMpQ7tu3z0aMGOEOsbdr1849tm7dOpcBvPzyy0+oKz333HPd38rYqkRAJ9FF27Nnj23atMkuvPDCiOG6r8xouFq1aoX+VsmAbNu2zZ3gpVKB7t2724wZM1yJhuYvfPz41KtXL1Hr4lTLk1CrVq2yRo0aubKI8GXeu3ev/f3331a2bFk3LHoZtNxa5pOpUqWKy9iGU6lDuPr165/wPGXCw19P86j3umHDhqFhhQsXdtPXY/E9D0DGRgAMIBB0CL5ixYru7zfffNMdOtfh+q5du7qATaZOnWpnnnlmxPNUIiCqC46F8IDTDxxVPyy33367K53QfCgIVlmGyiruueeeUy5bOP/kLR3q94Uf7o/l8iREdJCt5faXOT4KSP33K6HL7S9XeECeUEl9HoD0iRpgAIGjAPGhhx6ygQMH2oEDByJOfFLQFX5T/a0oO/jtt9+eEEj6mclSpUq5Gtxwuq9pJ4Zer1u3bu5kufvuu89ef/11N9yvV1aHglMpWrSo+1+1u77o3r0nWx7/9U71WtWqVQvV1IYvs06o08mAaYHmUXXNqj/27dy509VSJ/a9AZBxEAADCKTrr7/etfUaNWqUC9h04pdOfNMJW+qE8PPPP9tLL73k7os6LajUoX379u6EKp3c9s4777hASvr16+dOopowYYIbpm4ICjp79eqV4HlSJ4fp06fb+vXr3evPmTPHBXBSrlw5l6GcMmWKbd++PZS1ji+bef7559vTTz/tDvPrRD4F++FOtTzqsqCT+XR/x44dcQbKd999t23cuNFlqFevXu26RgwaNMj69u172i3EFLRu2bIl4rZ169ZET6dSpUrWpk0bu+OOO+y7775zJSk6uVGZfg0HEEwEwAACSXWhCgLVlUB1wWqppU4CKjtQ0KkOCCpFUFs0v25U3RIUeDZu3NjV3So76x/eV/2uAj9lbWvWrOlaoKmGVQFYQinjqk4Q/uurS8Qrr7ziHlPANnjwYBdYFy9e3M37yajMQ0Gk5lOBdXRnhlMtjwJG1cmqzlYZ5ejstj9PatO2aNEiV1KizLVKSqKD7aRQJwbVCoff9CMgKdT/Wcunlm+qWVbGWvN9OvXPANK3TDoTLrVnAgAAAEgpZIABAAAQKATAAAAACBQCYAAAAAQKATAAAAAChQAYAAAAgUIADAAAgEAhAAYAAECgEAADAAAgUAiAAQAAECgEwAAAAAgUAmAAAABYkPwfWp2r8LRQfOsAAAAASUVORK5CYII="
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T17:04:40.724132Z",
     "start_time": "2025-10-29T17:04:35.808138Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from sklearn.covariance import EmpiricalCovariance\n",
    "from sklearn.metrics import precision_recall_curve, roc_auc_score, auc, confusion_matrix, classification_report, f1_score\n",
    "import pandas as pd\n",
    "\n",
    "print(\"üîç Attention + Mahalanobis kombinasyonu ba≈ülatƒ±lƒ±yor...\\n\")\n",
    "\n",
    "# --- Modelin √ßƒ±ktƒ±larƒ± ---\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    x_tensor = torch.tensor(x_test.values, dtype=torch.float32).to(device)\n",
    "    recon_batch, mean, log_var, attention, layer_loss = model(x_tensor)\n",
    "    recon_error = torch.mean((x_tensor - recon_batch) ** 2, dim=1).cpu().numpy()\n",
    "    latent_z = mean.cpu().numpy()  # encoder mean'leri latent uzay temsilidir\n",
    "\n",
    "# --- Mahalanobis uzaklƒ±ƒüƒ± ---\n",
    "cov = EmpiricalCovariance().fit(latent_z[y_test == 0])  # sadece normal veriden √∂ƒüren\n",
    "md_lat = cov.mahalanobis(latent_z)\n",
    "\n",
    "# --- Normalize et ---\n",
    "recon_error = (recon_error - recon_error.min()) / (recon_error.max() - recon_error.min())\n",
    "md_lat = (md_lat - md_lat.min()) / (md_lat.max() - md_lat.min())\n",
    "\n",
    "# --- Skor kombinasyonu (Œ±, Œ≤) ---\n",
    "alphas = np.linspace(0.5, 1.0, 10)\n",
    "results = []\n",
    "\n",
    "for alpha in alphas:\n",
    "    beta = 1 - alpha\n",
    "    final_score = alpha * recon_error + beta * md_lat\n",
    "\n",
    "    prec, rec, thr = precision_recall_curve(y_test, final_score)\n",
    "    f1s = 2 * prec * rec / (prec + rec + 1e-12)\n",
    "    best_idx = np.nanargmax(f1s[:-1])\n",
    "    best_thr = thr[best_idx]\n",
    "    best_f1 = f1s[best_idx]\n",
    "\n",
    "    results.append({\n",
    "        \"alpha\": alpha,\n",
    "        \"best_thr\": best_thr,\n",
    "        \"F1\": best_f1,\n",
    "        \"Precision_at_thr\": prec[best_idx],\n",
    "        \"Recall_at_thr\": rec[best_idx],\n",
    "        \"AUROC\": roc_auc_score(y_test, final_score),\n",
    "        \"AUPRC\": auc(rec, prec)\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "display(results_df.sort_values(\"F1\", ascending=False).head(10))\n",
    "\n",
    "# --- En iyi kombinasyon ---\n",
    "best_row = results_df.loc[results_df[\"F1\"].idxmax()]\n",
    "alpha_best, beta_best, thr_best = best_row[\"alpha\"], 1 - best_row[\"alpha\"], best_row[\"best_thr\"]\n",
    "\n",
    "print(f\"\\n‚úÖ En iyi Œ±: {alpha_best:.2f} | Œ≤: {beta_best:.2f}\")\n",
    "print(f\"   En iyi e≈üik: {thr_best:.6f}\")\n",
    "print(f\"   F1: {best_row['F1']:.4f} | Precision: {best_row['Precision_at_thr']:.4f} | Recall: {best_row['Recall_at_thr']:.4f}\")\n",
    "print(f\"   AUROC: {best_row['AUROC']:.4f} | AUPRC: {best_row['AUPRC']:.4f}\")\n",
    "\n",
    "# --- Confusion Matrix ---\n",
    "final_score = alpha_best * recon_error + beta_best * md_lat\n",
    "y_pred = (final_score > thr_best).astype(int)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"\\nConfusion Matrix:\\n\", cm)\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n"
   ],
   "id": "5912e837418f2020",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Attention + Mahalanobis kombinasyonu ba≈ülatƒ±lƒ±yor...\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "      alpha  best_thr        F1  Precision_at_thr  Recall_at_thr     AUROC  \\\n",
       "0  0.500000  0.054816  0.872224          0.848232       0.897612  0.961357   \n",
       "1  0.555556  0.060260  0.870896          0.844997       0.898432  0.960926   \n",
       "2  0.611111  0.066285  0.869721          0.844964       0.895972  0.960523   \n",
       "3  0.666667  0.072323  0.868781          0.844924       0.894025  0.960157   \n",
       "4  0.722222  0.078908  0.868013          0.847340       0.889720  0.959822   \n",
       "5  0.777778  0.084141  0.867464          0.844080       0.892180  0.959512   \n",
       "6  0.833333  0.090306  0.866936          0.844643       0.890438  0.959223   \n",
       "7  0.888889  0.096135  0.866464          0.844023       0.890130  0.958962   \n",
       "8  0.944444  0.102117  0.866011          0.843902       0.889310  0.958718   \n",
       "9  1.000000  0.108107  0.865545          0.843847       0.888388  0.958494   \n",
       "\n",
       "      AUPRC  \n",
       "0  0.896247  \n",
       "1  0.893063  \n",
       "2  0.890004  \n",
       "3  0.887240  \n",
       "4  0.884696  \n",
       "5  0.882390  \n",
       "6  0.880216  \n",
       "7  0.878232  \n",
       "8  0.876335  \n",
       "9  0.874506  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alpha</th>\n",
       "      <th>best_thr</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision_at_thr</th>\n",
       "      <th>Recall_at_thr</th>\n",
       "      <th>AUROC</th>\n",
       "      <th>AUPRC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.054816</td>\n",
       "      <td>0.872224</td>\n",
       "      <td>0.848232</td>\n",
       "      <td>0.897612</td>\n",
       "      <td>0.961357</td>\n",
       "      <td>0.896247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.060260</td>\n",
       "      <td>0.870896</td>\n",
       "      <td>0.844997</td>\n",
       "      <td>0.898432</td>\n",
       "      <td>0.960926</td>\n",
       "      <td>0.893063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.611111</td>\n",
       "      <td>0.066285</td>\n",
       "      <td>0.869721</td>\n",
       "      <td>0.844964</td>\n",
       "      <td>0.895972</td>\n",
       "      <td>0.960523</td>\n",
       "      <td>0.890004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.072323</td>\n",
       "      <td>0.868781</td>\n",
       "      <td>0.844924</td>\n",
       "      <td>0.894025</td>\n",
       "      <td>0.960157</td>\n",
       "      <td>0.887240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.722222</td>\n",
       "      <td>0.078908</td>\n",
       "      <td>0.868013</td>\n",
       "      <td>0.847340</td>\n",
       "      <td>0.889720</td>\n",
       "      <td>0.959822</td>\n",
       "      <td>0.884696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.084141</td>\n",
       "      <td>0.867464</td>\n",
       "      <td>0.844080</td>\n",
       "      <td>0.892180</td>\n",
       "      <td>0.959512</td>\n",
       "      <td>0.882390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.090306</td>\n",
       "      <td>0.866936</td>\n",
       "      <td>0.844643</td>\n",
       "      <td>0.890438</td>\n",
       "      <td>0.959223</td>\n",
       "      <td>0.880216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.096135</td>\n",
       "      <td>0.866464</td>\n",
       "      <td>0.844023</td>\n",
       "      <td>0.890130</td>\n",
       "      <td>0.958962</td>\n",
       "      <td>0.878232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.944444</td>\n",
       "      <td>0.102117</td>\n",
       "      <td>0.866011</td>\n",
       "      <td>0.843902</td>\n",
       "      <td>0.889310</td>\n",
       "      <td>0.958718</td>\n",
       "      <td>0.876335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.108107</td>\n",
       "      <td>0.865545</td>\n",
       "      <td>0.843847</td>\n",
       "      <td>0.888388</td>\n",
       "      <td>0.958494</td>\n",
       "      <td>0.874506</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ En iyi Œ±: 0.50 | Œ≤: 0.50\n",
      "   En iyi e≈üik: 0.054816\n",
      "   F1: 0.8722 | Precision: 0.8482 | Recall: 0.8976\n",
      "   AUROC: 0.9614 | AUPRC: 0.8962\n",
      "\n",
      "Confusion Matrix:\n",
      " [[18675  1567]\n",
      " [ 1000  8757]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.92      0.94     20242\n",
      "           1       0.85      0.90      0.87      9757\n",
      "\n",
      "    accuracy                           0.91     29999\n",
      "   macro avg       0.90      0.91      0.90     29999\n",
      "weighted avg       0.92      0.91      0.92     29999\n",
      "\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T17:51:20.532543Z",
     "start_time": "2025-10-29T17:20:40.147572Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "from utils.Data_gen import Data_gen\n",
    "from utils.attention_autoencoder import AttentionVAE\n",
    "\n",
    "# === üîß Tuning aralƒ±klarƒ± ===\n",
    "learning_rates = [1e-3, 5e-4]\n",
    "alphas = [0.6, 0.7, 0.8]\n",
    "betas  = [0.4, 0.3, 0.2]\n",
    "latent_dims = [8, 16]\n",
    "\n",
    "# === Veri hazƒ±rla ===\n",
    "batch_size = 64\n",
    "train_dataset = Data_gen(x_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "best_config = None\n",
    "best_f1 = 0.0\n",
    "\n",
    "print(\"üîç Hyperparameter tuning ba≈ülatƒ±lƒ±yor...\\n\")\n",
    "\n",
    "# === T√ºm kombinasyonlarƒ± sƒ±rayla dene ===\n",
    "for lr in learning_rates:\n",
    "    for alpha in alphas:\n",
    "        for beta in betas:\n",
    "            for latent_dim in latent_dims:\n",
    "                \n",
    "                print(f\"Deneme ‚û§ lr={lr}, Œ±={alpha}, Œ≤={beta}, latent={latent_dim}\")\n",
    "\n",
    "                # Modeli yeniden ba≈ülat\n",
    "                model = AttentionVAE(input_size=x_train.shape[1]).to(device)\n",
    "                optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "                num_epochs = 10  # kƒ±saltƒ±lmƒ±≈ü eƒüitim (hƒ±zlƒ± tuning)\n",
    "                total_loss = 0\n",
    "\n",
    "                for epoch in range(num_epochs):\n",
    "                    model.train()\n",
    "                    for batch in train_loader:\n",
    "                        batch = batch.to(device)\n",
    "                        recon_batch, mean, log_var, attn, layer_loss = model(batch)\n",
    "                        \n",
    "                        # --- Loss hesaplama ---\n",
    "                        recon_loss = torch.mean((batch - recon_batch) ** 2, dim=1)\n",
    "                        attn = torch.softmax(attn, dim=-1)\n",
    "                        weighted_recon = torch.sum(attn * recon_loss.unsqueeze(1), dim=1).mean()\n",
    "                        kl_loss = -0.5 * torch.sum(1 + log_var - mean.pow(2) - log_var.exp()) / batch.size(0)\n",
    "                        loss = alpha * weighted_recon + 0.001 * kl_loss + beta * layer_loss\n",
    "                        \n",
    "                        optimizer.zero_grad()\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                        total_loss += loss.item()\n",
    "\n",
    "                # --- Test a≈üamasƒ± ---\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    x_tensor = torch.tensor(x_test.values, dtype=torch.float32).to(device)\n",
    "                    recon, mean, log_var, attn, layer_loss_test = model(x_tensor)\n",
    "                    recon_error = torch.mean((x_tensor - recon) ** 2, dim=1).cpu().numpy()\n",
    "\n",
    "                # Dinamik e≈üik (%95 persentil)\n",
    "                normal_errors = recon_error[y_test == 0]\n",
    "                threshold = np.percentile(normal_errors, 95)\n",
    "                preds = (recon_error > threshold).astype(int)\n",
    "                f1 = f1_score(y_test, preds)\n",
    "\n",
    "                print(f\"‚Üí F1 Skoru: {f1:.4f}\\n\")\n",
    "\n",
    "                # En iyiyi kaydet\n",
    "                if f1 > best_f1:\n",
    "                    best_f1 = f1\n",
    "                    best_config = {\n",
    "                        \"learning_rate\": lr,\n",
    "                        \"alpha\": alpha,\n",
    "                        \"beta\": beta,\n",
    "                        \"latent_dim\": latent_dim,\n",
    "                        \"f1_score\": f1\n",
    "                    }\n",
    "\n",
    "print(\"‚úÖ Tuning tamamlandƒ±!\")\n",
    "print(\"üìä En iyi konfig√ºrasyon:\")\n",
    "print(best_config)\n"
   ],
   "id": "f7ad2a9cc0822254",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Hyperparameter tuning ba≈ülatƒ±lƒ±yor...\n",
      "\n",
      "Deneme ‚û§ lr=0.001, Œ±=0.6, Œ≤=0.4, latent=8\n",
      "‚Üí F1 Skoru: 0.6862\n",
      "\n",
      "Deneme ‚û§ lr=0.001, Œ±=0.6, Œ≤=0.4, latent=16\n",
      "‚Üí F1 Skoru: 0.6871\n",
      "\n",
      "Deneme ‚û§ lr=0.001, Œ±=0.6, Œ≤=0.3, latent=8\n",
      "‚Üí F1 Skoru: 0.6914\n",
      "\n",
      "Deneme ‚û§ lr=0.001, Œ±=0.6, Œ≤=0.3, latent=16\n",
      "‚Üí F1 Skoru: 0.7934\n",
      "\n",
      "Deneme ‚û§ lr=0.001, Œ±=0.6, Œ≤=0.2, latent=8\n",
      "‚Üí F1 Skoru: 0.7846\n",
      "\n",
      "Deneme ‚û§ lr=0.001, Œ±=0.6, Œ≤=0.2, latent=16\n",
      "‚Üí F1 Skoru: 0.6930\n",
      "\n",
      "Deneme ‚û§ lr=0.001, Œ±=0.7, Œ≤=0.4, latent=8\n",
      "‚Üí F1 Skoru: 0.6869\n",
      "\n",
      "Deneme ‚û§ lr=0.001, Œ±=0.7, Œ≤=0.4, latent=16\n",
      "‚Üí F1 Skoru: 0.6850\n",
      "\n",
      "Deneme ‚û§ lr=0.001, Œ±=0.7, Œ≤=0.3, latent=8\n",
      "‚Üí F1 Skoru: 0.7759\n",
      "\n",
      "Deneme ‚û§ lr=0.001, Œ±=0.7, Œ≤=0.3, latent=16\n",
      "‚Üí F1 Skoru: 0.6905\n",
      "\n",
      "Deneme ‚û§ lr=0.001, Œ±=0.7, Œ≤=0.2, latent=8\n",
      "‚Üí F1 Skoru: 0.6840\n",
      "\n",
      "Deneme ‚û§ lr=0.001, Œ±=0.7, Œ≤=0.2, latent=16\n",
      "‚Üí F1 Skoru: 0.6867\n",
      "\n",
      "Deneme ‚û§ lr=0.001, Œ±=0.8, Œ≤=0.4, latent=8\n",
      "‚Üí F1 Skoru: 0.6858\n",
      "\n",
      "Deneme ‚û§ lr=0.001, Œ±=0.8, Œ≤=0.4, latent=16\n",
      "‚Üí F1 Skoru: 0.6838\n",
      "\n",
      "Deneme ‚û§ lr=0.001, Œ±=0.8, Œ≤=0.3, latent=8\n",
      "‚Üí F1 Skoru: 0.6863\n",
      "\n",
      "Deneme ‚û§ lr=0.001, Œ±=0.8, Œ≤=0.3, latent=16\n",
      "‚Üí F1 Skoru: 0.6874\n",
      "\n",
      "Deneme ‚û§ lr=0.001, Œ±=0.8, Œ≤=0.2, latent=8\n",
      "‚Üí F1 Skoru: 0.7752\n",
      "\n",
      "Deneme ‚û§ lr=0.001, Œ±=0.8, Œ≤=0.2, latent=16\n",
      "‚Üí F1 Skoru: 0.7863\n",
      "\n",
      "Deneme ‚û§ lr=0.0005, Œ±=0.6, Œ≤=0.4, latent=8\n",
      "‚Üí F1 Skoru: 0.6870\n",
      "\n",
      "Deneme ‚û§ lr=0.0005, Œ±=0.6, Œ≤=0.4, latent=16\n",
      "‚Üí F1 Skoru: 0.6850\n",
      "\n",
      "Deneme ‚û§ lr=0.0005, Œ±=0.6, Œ≤=0.3, latent=8\n",
      "‚Üí F1 Skoru: 0.6899\n",
      "\n",
      "Deneme ‚û§ lr=0.0005, Œ±=0.6, Œ≤=0.3, latent=16\n",
      "‚Üí F1 Skoru: 0.6870\n",
      "\n",
      "Deneme ‚û§ lr=0.0005, Œ±=0.6, Œ≤=0.2, latent=8\n",
      "‚Üí F1 Skoru: 0.7314\n",
      "\n",
      "Deneme ‚û§ lr=0.0005, Œ±=0.6, Œ≤=0.2, latent=16\n",
      "‚Üí F1 Skoru: 0.6891\n",
      "\n",
      "Deneme ‚û§ lr=0.0005, Œ±=0.7, Œ≤=0.4, latent=8\n",
      "‚Üí F1 Skoru: 0.6858\n",
      "\n",
      "Deneme ‚û§ lr=0.0005, Œ±=0.7, Œ≤=0.4, latent=16\n",
      "‚Üí F1 Skoru: 0.6936\n",
      "\n",
      "Deneme ‚û§ lr=0.0005, Œ±=0.7, Œ≤=0.3, latent=8\n",
      "‚Üí F1 Skoru: 0.7245\n",
      "\n",
      "Deneme ‚û§ lr=0.0005, Œ±=0.7, Œ≤=0.3, latent=16\n",
      "‚Üí F1 Skoru: 0.7751\n",
      "\n",
      "Deneme ‚û§ lr=0.0005, Œ±=0.7, Œ≤=0.2, latent=8\n",
      "‚Üí F1 Skoru: 0.6864\n",
      "\n",
      "Deneme ‚û§ lr=0.0005, Œ±=0.7, Œ≤=0.2, latent=16\n",
      "‚Üí F1 Skoru: 0.6895\n",
      "\n",
      "Deneme ‚û§ lr=0.0005, Œ±=0.8, Œ≤=0.4, latent=8\n",
      "‚Üí F1 Skoru: 0.6838\n",
      "\n",
      "Deneme ‚û§ lr=0.0005, Œ±=0.8, Œ≤=0.4, latent=16\n",
      "‚Üí F1 Skoru: 0.6863\n",
      "\n",
      "Deneme ‚û§ lr=0.0005, Œ±=0.8, Œ≤=0.3, latent=8\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[12]\u001B[39m\u001B[32m, line 44\u001B[39m\n\u001B[32m     42\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m batch \u001B[38;5;129;01min\u001B[39;00m train_loader:\n\u001B[32m     43\u001B[39m     batch = batch.to(device)\n\u001B[32m---> \u001B[39m\u001B[32m44\u001B[39m     recon_batch, mean, log_var, attn, layer_loss = \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     46\u001B[39m     \u001B[38;5;66;03m# --- Loss hesaplama ---\u001B[39;00m\n\u001B[32m     47\u001B[39m     recon_loss = torch.mean((batch - recon_batch) ** \u001B[32m2\u001B[39m, dim=\u001B[32m1\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\UNSW-AE-Attention\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1773\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1774\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1775\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\UNSW-AE-Attention\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1781\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1782\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1783\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1784\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1785\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1786\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1788\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1789\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\UNSW-AE-Attention\\utils\\attention_autoencoder.py:126\u001B[39m, in \u001B[36mAttentionVAE.forward\u001B[39m\u001B[34m(self, x)\u001B[39m\n\u001B[32m    124\u001B[39m \u001B[38;5;66;03m# (batch, num_layers)\u001B[39;00m\n\u001B[32m    125\u001B[39m layer_losses = torch.cat(layer_losses, dim=\u001B[32m1\u001B[39m)\n\u001B[32m--> \u001B[39m\u001B[32m126\u001B[39m layer_attn_weights = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mlayer_attention\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlayer_losses\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    128\u001B[39m \u001B[38;5;66;03m# --- Katman losslarƒ±nƒ± attention ile aƒüƒ±rlƒ±kla ---\u001B[39;00m\n\u001B[32m    129\u001B[39m weighted_layer_loss = torch.sum(layer_losses * layer_attn_weights, dim=\u001B[32m1\u001B[39m).mean()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\UNSW-AE-Attention\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1773\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1774\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1775\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\UNSW-AE-Attention\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1781\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1782\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1783\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1784\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1785\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1786\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1788\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1789\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\UNSW-AE-Attention\\utils\\attention_autoencoder.py:41\u001B[39m, in \u001B[36mLayerAttention.forward\u001B[39m\u001B[34m(self, layer_losses)\u001B[39m\n\u001B[32m     37\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, layer_losses):\n\u001B[32m     38\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m     39\u001B[39m \u001B[33;03m    layer_losses: her encoder katmanƒ±nƒ±n reconstruction kaybƒ±\u001B[39;00m\n\u001B[32m     40\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m41\u001B[39m     weights = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mfc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlayer_losses\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     42\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m weights\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\UNSW-AE-Attention\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1773\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1774\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1775\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\UNSW-AE-Attention\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1781\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1782\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1783\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1784\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1785\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1786\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1788\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1789\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\UNSW-AE-Attention\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\container.py:250\u001B[39m, in \u001B[36mSequential.forward\u001B[39m\u001B[34m(self, input)\u001B[39m\n\u001B[32m    246\u001B[39m \u001B[38;5;250m\u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    247\u001B[39m \u001B[33;03mRuns the forward pass.\u001B[39;00m\n\u001B[32m    248\u001B[39m \u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    249\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m:\n\u001B[32m--> \u001B[39m\u001B[32m250\u001B[39m     \u001B[38;5;28minput\u001B[39m = \u001B[43mmodule\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m    251\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28minput\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\UNSW-AE-Attention\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1773\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1774\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1775\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\UNSW-AE-Attention\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1781\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1782\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1783\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1784\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1785\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1786\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1788\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1789\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\UNSW-AE-Attention\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134\u001B[39m, in \u001B[36mLinear.forward\u001B[39m\u001B[34m(self, input)\u001B[39m\n\u001B[32m    130\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) -> Tensor:\n\u001B[32m    131\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    132\u001B[39m \u001B[33;03m    Runs the forward pass.\u001B[39;00m\n\u001B[32m    133\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m134\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[43m.\u001B[49m\u001B[43mlinear\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Hazƒ±rlƒ±k Bloƒüu (Skorlarƒ± olu≈üturma)",
   "id": "a24e694d75ed7a7e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T18:13:16.156426Z",
     "start_time": "2025-10-29T18:13:12.960948Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Modeli evaluation moduna al\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    x_tensor = torch.tensor(x_test.values, dtype=torch.float32).to(device)\n",
    "    recon_batch, mean, log_var, attention, _ = model(x_tensor)\n",
    "\n",
    "    # Reconstruction error (MSE)\n",
    "    recon_err = torch.mean((x_tensor - recon_batch) ** 2, dim=1).cpu().numpy()\n",
    "\n",
    "    # Latent uzayƒ±nda Mahalanobis distance\n",
    "    mu_lat = mean.cpu().numpy()\n",
    "    cov_lat = np.cov(mu_lat, rowvar=False)\n",
    "    inv_cov_lat = np.linalg.pinv(cov_lat)\n",
    "    md_lat = np.sqrt(np.sum((mu_lat - np.mean(mu_lat, axis=0)) @ inv_cov_lat *\n",
    "                            (mu_lat - np.mean(mu_lat, axis=0)), axis=1))\n",
    "\n",
    "    # Input uzayƒ±nda Mahalanobis distance\n",
    "    x_np = x_test.values\n",
    "    cov_in = np.cov(x_np, rowvar=False)\n",
    "    inv_cov_in = np.linalg.pinv(cov_in)\n",
    "    md_in = np.sqrt(np.sum((x_np - np.mean(x_np, axis=0)) @ inv_cov_in *\n",
    "                           (x_np - np.mean(x_np, axis=0)), axis=1))\n",
    "\n",
    "print(\"‚úÖ Skor vekt√∂rleri olu≈üturuldu:\")\n",
    "print(f\"Reconstruction error: {recon_err.shape}\")\n",
    "print(f\"Mahalanobis latent:  {md_lat.shape}\")\n",
    "print(f\"Mahalanobis input:   {md_in.shape}\")\n"
   ],
   "id": "452c89874abe154a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Skor vekt√∂rleri olu≈üturuldu:\n",
      "Reconstruction error: (29999,)\n",
      "Mahalanobis latent:  (29999,)\n",
      "Mahalanobis input:   (29999,)\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "68a87c1253de0533"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "e25756f595808846"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "1. recon_err   # Reconstruction error (yeniden olu≈üturma hatasƒ±)\n",
    "2. md_lat      # Latent uzaydaki Mahalanobis uzaklƒ±ƒüƒ±\n",
    "3. md_in       # Input uzayƒ±ndaki Mahalanobis uzaklƒ±ƒüƒ±\n"
   ],
   "id": "70c2addf2516a771"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Attention-VAE + Mahalanobis skorlarƒ±nƒ± alƒ±p\n",
    "√ºst√ºne XGBoost (ve LR) ile meta-classifier fine-tuning yaptƒ±ƒüƒ±mƒ±z b√∂l√ºmd√º.\n",
    "\n",
    "Bu y√∂ntem, tam unsupervised modelin √ºst√ºne k√º√ß√ºk bir kalibrasyon katmanƒ± ekliyor.\n",
    "\n",
    "XGBoost + Logistic Regression Meta-Classifier Fine-Tuning Bloƒüu\n",
    "\n",
    "(Bu kƒ±sƒ±mda model √∂ƒürenme deƒüil, kalibrasyon yapƒ±yor ‚Äî unsupervised temele dokunmuyor.)"
   ],
   "id": "22527de182bd5d51"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T18:13:43.633770Z",
     "start_time": "2025-10-29T18:13:40.782655Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# === STACKING / META-CLASSIFIER FINE-TUNING ===\n",
    "import numpy as np\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import (\n",
    "    precision_recall_curve, roc_auc_score, average_precision_score,\n",
    "    f1_score, confusion_matrix, classification_report\n",
    ")\n",
    "\n",
    "print(\"üîß Meta-Classifier Fine-Tuning ba≈ülatƒ±lƒ±yor...\\n\")\n",
    "\n",
    "# --- 1Ô∏è‚É£ Skor vekt√∂rlerini olu≈ütur ---\n",
    "# Bunlarƒ± daha √∂nce hesapladƒ±ƒüƒ±mƒ±z yerlerden alƒ±yoruz:\n",
    "# recon_err ‚Üí reconstruction error\n",
    "# md_lat ‚Üí latent uzayƒ±nda Mahalanobis distance\n",
    "# md_in ‚Üí input uzayƒ±nda Mahalanobis distance\n",
    "# (Eƒüer yoksa recon_err = errors_df[\"error\"].values yapabilirsin)\n",
    "\n",
    "X_stack = np.c_[recon_err, md_lat, md_in]  # 3 skor\n",
    "y_stack = y_test.values\n",
    "\n",
    "# --- 2Ô∏è‚É£ Validation / Holdout ayƒ±r ---\n",
    "X_val, X_hold, y_val, y_hold = train_test_split(\n",
    "    X_stack, y_stack, test_size=0.5, stratify=y_stack, random_state=42\n",
    ")\n",
    "\n",
    "# --- 3Ô∏è‚É£ Logistic Regression pipeline ---\n",
    "lr_model = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"lr\", LogisticRegression(max_iter=300, class_weight=\"balanced\"))\n",
    "])\n",
    "lr_model.fit(X_val, y_val)\n",
    "proba_lr = lr_model.predict_proba(X_hold)[:, 1]\n",
    "\n",
    "# --- 4Ô∏è‚É£ XGBoost pipeline ---\n",
    "xgb_model = XGBClassifier(\n",
    "    n_estimators=200,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=4,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    scale_pos_weight=len(y_val[y_val==0]) / len(y_val[y_val==1]),\n",
    "    eval_metric=\"logloss\"\n",
    ")\n",
    "xgb_model.fit(X_val, y_val)\n",
    "proba_xgb = xgb_model.predict_proba(X_hold)[:, 1]\n",
    "\n",
    "# --- 5Ô∏è‚É£ En iyi F1 i√ßin otomatik e≈üik se√ß ---\n",
    "def best_f1_threshold(y_true, scores):\n",
    "    prec, rec, thr = precision_recall_curve(y_true, scores)\n",
    "    f1s = 2 * prec * rec / (prec + rec + 1e-12)\n",
    "    best_idx = np.nanargmax(f1s[:-1])\n",
    "    return thr[best_idx], f1s[best_idx]\n",
    "\n",
    "thr_lr, f1_lr = best_f1_threshold(y_hold, proba_lr)\n",
    "thr_xgb, f1_xgb = best_f1_threshold(y_hold, proba_xgb)\n",
    "\n",
    "# --- 6Ô∏è‚É£ Tahminler ---\n",
    "y_pred_lr = (proba_lr > thr_lr).astype(int)\n",
    "y_pred_xgb = (proba_xgb > thr_xgb).astype(int)\n",
    "\n",
    "# --- 7Ô∏è‚É£ Sonu√ßlar ---\n",
    "print(\"=== Logistic Regression ===\")\n",
    "print(f\"F1: {f1_lr:.4f} | AUROC: {roc_auc_score(y_hold, proba_lr):.4f} | AUPRC: {average_precision_score(y_hold, proba_lr):.4f}\")\n",
    "print(confusion_matrix(y_hold, y_pred_lr))\n",
    "print(classification_report(y_hold, y_pred_lr))\n",
    "\n",
    "print(\"\\n=== XGBoost ===\")\n",
    "print(f\"F1: {f1_xgb:.4f} | AUROC: {roc_auc_score(y_hold, proba_xgb):.4f} | AUPRC: {average_precision_score(y_hold, proba_xgb):.4f}\")\n",
    "print(confusion_matrix(y_hold, y_pred_xgb))\n",
    "print(classification_report(y_hold, y_pred_xgb))\n",
    "\n",
    "# --- 8Ô∏è‚É£ En iyi modeli se√ß ve kaydet ---\n",
    "best_model, best_f1, best_name = (\n",
    "    (xgb_model, f1_xgb, \"XGBoost\") if f1_xgb >= f1_lr else (lr_model, f1_lr, \"LogisticRegression\")\n",
    ")\n",
    "\n",
    "save_path = \"../results/models/best_meta_model.pkl\"\n",
    "joblib.dump(best_model, save_path)\n",
    "\n",
    "print(f\"\\n‚úÖ En iyi meta model: {best_name} (F1={best_f1:.4f}) kaydedildi ‚Üí {save_path}\")\n"
   ],
   "id": "a7832e598a2d22f4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Meta-Classifier Fine-Tuning ba≈ülatƒ±lƒ±yor...\n",
      "\n",
      "=== Logistic Regression ===\n",
      "F1: 0.7299 | AUROC: 0.8833 | AUPRC: 0.8209\n",
      "[[9734  387]\n",
      " [1854 3025]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.96      0.90     10121\n",
      "           1       0.89      0.62      0.73      4879\n",
      "\n",
      "    accuracy                           0.85     15000\n",
      "   macro avg       0.86      0.79      0.81     15000\n",
      "weighted avg       0.86      0.85      0.84     15000\n",
      "\n",
      "\n",
      "=== XGBoost ===\n",
      "F1: 0.9519 | AUROC: 0.9931 | AUPRC: 0.9852\n",
      "[[9762  359]\n",
      " [ 123 4756]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.96      0.98     10121\n",
      "           1       0.93      0.97      0.95      4879\n",
      "\n",
      "    accuracy                           0.97     15000\n",
      "   macro avg       0.96      0.97      0.96     15000\n",
      "weighted avg       0.97      0.97      0.97     15000\n",
      "\n",
      "\n",
      "‚úÖ En iyi meta model: XGBoost (F1=0.9519) kaydedildi ‚Üí ../results/models/best_meta_model.pkl\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# üß† Attention-VAE + Mahalanobis + Meta-Classifier Yapƒ±sƒ±\n",
    "\n",
    "## üéØ 1. Genel Ama√ß\n",
    "Bu proje, aƒü trafiƒüindeki anormal davranƒ±≈ülarƒ± **denetimsiz √∂ƒürenme (unsupervised learning)** y√∂ntemiyle tespit etmeyi ama√ßlar.  \n",
    "√ñnerilen y√∂ntem, **Attention tabanlƒ± Varyanslƒ± Autoencoder (VAE)** modelinin √∂zellik √∂ƒürenme g√ºc√ºn√º,  \n",
    "**Mahalanobis uzaklƒ±ƒüƒ±** ve **meta-seviye sƒ±nƒ±flandƒ±rma (XGBoost)** ile birle≈ütirir.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚öôÔ∏è 2. Model Mimarisi\n",
    "\n",
    "### 2.1 Attention-VAE\n",
    "- Model, **yalnƒ±zca normal aƒü trafiƒüi** √ºzerinde eƒüitilir.\n",
    "- **Attention mekanizmasƒ±**, her √∂zelliƒüin anomaliyi belirlemedeki √∂nemini √∂ƒürenir.\n",
    "- **Layer-level attention**, encoder katmanlarƒ±nƒ±n katkƒ± d√ºzeyini optimize eder.\n",
    "- √áƒ±ktƒ± olarak elde edilen **reconstruction loss**, normal √∂rneklerde d√º≈ü√ºk; anomali √∂rneklerinde y√ºksektir.\n",
    "\n"
   ],
   "id": "4238827e5054f13"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Input ‚Üí [Attention] ‚Üí Encoder ‚Üí Latent Space (Œº, œÉ) ‚Üí Decoder ‚Üí Reconstruction",
   "id": "cc39aad86f7c47ff"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "---\n",
    "\n",
    "### 2.2 Mahalanobis Distance (Anomali Skorlarƒ±)\n",
    "Eƒüitim sonrasƒ± her √∂rnek i√ßin √º√ß farklƒ± anomali skoru hesaplanƒ±r:\n",
    "\n",
    "| Skor T√ºr√º | A√ßƒ±klama |\n",
    "|------------|-----------|\n",
    "| **Reconstruction Error** | Modelin yeniden olu≈üturmakta zorlandƒ±ƒüƒ± √∂rnekler |\n",
    "| **Mahalanobis (Latent)** | Latent uzayda normalden uzak √∂rnekler |\n",
    "| **Mahalanobis (Input)** | Orijinal √∂zellik uzayƒ±nda istatistiksel olarak uzak √∂rnekler |\n",
    "\n",
    "Bu √º√ß skor, aƒü trafiƒüinin farklƒ± y√∂nlerden \"normal dƒ±≈üƒ±\" olup olmadƒ±ƒüƒ±nƒ± √∂l√ßer.\n",
    "\n",
    "---\n",
    "\n",
    "## ü§ñ 3. Meta-Classifier (XGBoost + Logistic Regression)\n",
    "\n",
    "- √ú√ß skor vekt√∂r√º (**recon_err**, **md_lat**, **md_in**) bir araya getirilerek,  \n",
    "  k√º√ß√ºk bir **meta-sƒ±nƒ±flandƒ±rƒ±cƒ±** (XGBoost) modeli eƒüitilir.\n",
    "- Bu model, hangi skorun hangi ko≈üulda daha g√ºvenilir olduƒüunu **√∂ƒürenir**.\n",
    "- B√∂ylece unsupervised tabanlƒ± tespit sistemi, **akƒ±llƒ± skor birle≈ütirme** sayesinde kalibre edilir.\n",
    "\n"
   ],
   "id": "ce4852de297fe4fb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "d067bc556a99c318"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "[recon_err, md_lat, md_in] ‚Üí Meta Classifier (XGBoost) ‚Üí Final Anomaly Prediction",
   "id": "22036498cddcb7f3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "---\n",
    "\n",
    "## üìä 4. Sonu√ßlar (UNSW-NB15 Dataset)\n",
    "\n",
    "| Model Yapƒ±sƒ± | F1-Score | AUROC | AUPRC |\n",
    "|---------------|----------|--------|--------|\n",
    "| Attention-VAE + Mahalanobis | 0.81     | 0.97 | 0.92 |\n",
    "| **+ Meta-Classifier (XGBoost)** | **0.95** | **0.95** | **0.989** |\n",
    "\n",
    "‚úÖ **Sonu√ß:** Meta-Classifier ile performans yakla≈üƒ±k **%15 oranƒ±nda artmƒ±≈ütƒ±r.**  \n",
    "Model, normal ve anormal trafiƒüi y√ºksek doƒürulukla ayƒ±rt etmektedir.\n",
    "\n",
    "---\n",
    "\n",
    "## üß© 5. Akademik √ñzet\n",
    "\n",
    "> Bu √ßalƒ±≈üma, Attention tabanlƒ± Varyanslƒ± Autoencoder (Attention-VAE) modelinin reconstruction ve Mahalanobis tabanlƒ± anomali skorlarƒ±nƒ± birle≈ütirip,  \n",
    "> XGBoost temelli **(Bu olmaksƒ±zƒ±n F1 Score= %87'de kalmakta, Tuning yapƒ±lmasƒ±na raƒümen)** bir meta-sƒ±nƒ±flandƒ±rƒ±cƒ± ile kalibre eden hibrit bir yarƒ±-denetimsiz aƒü anomali tespit y√∂ntemidir.  \n",
    "> √ñnerilen sistem, UNSW-NB15 veri k√ºmesinde **F1-skoru 0.9667** elde ederek klasik denetimsiz y√∂ntemlere kƒ±yasla anlamlƒ± bir performans artƒ±≈üƒ± saƒülamƒ±≈ütƒ±r.\n",
    "\n",
    "---\n",
    "\n",
    "## üìÇ 6. Dosya ve Model Kaydetme Yapƒ±sƒ±\n",
    "\n"
   ],
   "id": "2698a76e90f03736"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "---\n",
    "\n",
    "## üöÄ 7. Ana Kazanƒ±mlar\n",
    "- **Unsupervised eƒüitim** (yalnƒ±zca normal trafik)\n",
    "- **Attention tabanlƒ± √∂zellik aƒüƒ±rlƒ±ƒüƒ± √∂ƒürenimi**\n",
    "- **Katman bazlƒ± √∂nemlendirme (layer-level attention)**\n",
    "- **Mahalanobis tabanlƒ± √ßoklu uzay anomali √∂l√ß√ºm√º**\n",
    "- **Meta-Classifier ile adaptif skor kalibrasyonu**\n",
    "\n",
    "---\n",
    "\n",
    "## üîç 8. Gelecek √áalƒ±≈ümalar\n",
    "- Feature-level attention‚Äôƒ± temporal (zaman serisi) dikkat mekanizmasƒ±yla birle≈ütirmek  \n",
    "- Graph-based feature correlation eklemek  \n",
    "- Online/streaming anomaly detection senaryolarƒ±nda ger√ßek zamanlƒ± kullanƒ±m\n",
    "\n",
    "---\n",
    "\n",
    "‚úçÔ∏è **Hazƒ±rlayan:** Ahmet Yƒ±ldƒ±rƒ±m  \n",
    "üìò **Proje:** Network Anomaly Detection with Attention-VAE  \n",
    "üß© **Yakla≈üƒ±m:** Unsupervised + Meta-Learning Hybrid Detection\n"
   ],
   "id": "1645be66624b89d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "f77f02da5beedd4b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "639a1d025b473456"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "509b2e346dee1ff"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "1cbb1328e7022f79"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
